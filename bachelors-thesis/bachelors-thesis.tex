\documentclass[12pt, a4paper, openright, twoside]{tiarbeit}


\usepackage{helvet}
\usepackage{subcaption}
\usepackage{tabularx}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{multirow}
%\usepackage{floatrow}
\usepackage{adjustbox}
\usepackage{layout}
\usepackage{fixme} % To create notes in draft mode
\fxsetup{theme=color}
% Use with pdflatex
\usepackage[hidelinks]{hyperref}
\pdfminorversion=7
% Use with latex
%\usepackage[dvipdfmx, hidelinks]{hyperref}
\usepackage{todonotes}
%\usepackage[ruled]{algorithm2e}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[outputdir=output]{minted}
\usepackage{appendix}
\usepackage{float}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\usepackage{caption}
%\captionsetup{justification=raggedright,singlelinecheck=false}
\newenvironment{code}{\captionsetup{type=listing}}{}
\usepackage{pgfplotstable}
\usepackage{xcolor}
\definecolor{LightGray}{gray}{0.93}

\usetikzlibrary{patterns}

\renewcommand{\floatpagefraction}{.8}%

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother

\pgfplotsset{
    discard if/.style 2 args={
        x filter/.append code={
            \edef\tempa{\thisrow{#1}}
            \edef\tempb{#2}
            \ifx\tempa\tempb
                \def\pgfmathresult{inf}
            \fi
        }
    },
    discard if not/.style 2 args={
        x filter/.append code={
            \edef\tempa{\thisrow{#1}}
            \edef\tempb{#2}
            \ifx\tempa\tempb
            \else
                \def\pgfmathresult{inf}
            \fi
        }
    }
}

\pgfplotsset{compat=1.18}

\graphicspath{{material/}} %{./}

\mathtoolsset{centercolon}

\makeatletter
\newcommand{\Addlegendentry}[2][]{
\ifx\pgfplots@currentplot@firstcoord@x\pgfutil@empty\else\addlegendentry[#1]{#2}\fi
}
\makeatother

\newcommand{\threshMinipage}[3]{
\begin{minipage}{0.49\textwidth}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ylabel={time per element [ns]},
                xlabel={threshold},
                height=0.4\textheight,
                width=\textwidth,
                xminorgrids=true,
                xmajorgrids=true,
                yminorgrids=true,
                ymajorgrids=true,
                xmode=log,
                log basis x=2,
                %xmin=2,
                ymax=#2,
                xtick={2^1,2^2,2^3,2^4,2^5,2^6,2^7,2^8,2^9},
                legend style={fill=white, fill opacity=0.6, draw opacity=1,text opacity=1,font=\footnotesize},
                legend cell align={left},
                legend pos= #3 west,
            ]
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-float-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-float-int32-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-double-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-double-int64-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int8-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int16-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int32-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int64-#1-Uniform.dat};
            \legend{\texttt{float}, \texttt{float-int32}, \texttt{double}, \texttt{double-int64}, \texttt{int8}, \texttt{int16}, \texttt{int32}, \texttt{int64}}
        \end{axis}
    \end{tikzpicture}
    \caption{Runtime of #1 for different values for \texttt{cmpSortThreshold}, $2^{18}$ elements, Uniform distribution}
    \label{fig:cmpThresh-#1}
\end{minipage}
}

\newcommand{\threshFigOneReg}[2]{
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ylabel={time per element [ns]},
                xlabel={threshold},
                height=0.4\textheight,
                width=0.5\textwidth,
                xminorgrids=true,
                xmajorgrids=true,
                yminorgrids=true,
                ymajorgrids=true,
                xmode=log,
                log basis x=2,
                %xmin=2,
                ymax=#2,
                xtick={2^1,2^2,2^3,2^4,2^5,2^6,2^7,2^8,2^9},
                legend style={fill=white, fill opacity=0.6, draw opacity=1,text opacity=1},
                legend cell align={left},
                legend pos= north west,
            ]

            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-float-int64-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int8-int64-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int16-int64-#1-Uniform.dat};
            \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int32-int64-#1-Uniform.dat};

            \legend{\texttt{float-int64}, \texttt{int8-int64}, \texttt{int16-int64}, \texttt{int32-int64}}
        \end{axis}
    \end{tikzpicture}
    \caption{#1}
    \label{fig:cmpThresh-#1}
\end{figure}
}

\newcommand{\tpeFig}[2]{
\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ylabel={time per element [ns]},
                xlabel={number of elements},
                height=\textwidth,
                width=0.9\textwidth,
                xminorgrids=true,
                xmajorgrids=true,
                %yminorgrids=true,
                ymajorgrids=true,
                xmode=log,
                ymode=log,
                log basis x=2,
                %legend pos=outer north east,
                legend style={fill=white, fill opacity=0.6, draw opacity=1,text opacity=1,font=\small},
                legend cell align={left},
            ]
            \addplot table[x=number_of_elements, y=MoellerSeq] {data/tpe-#1-#2.dat};
            \Addlegendentry{MoellerSeq}
            \addplot table[x=number_of_elements, y=RadixSeq] {data/tpe-#1-#2.dat};
            \Addlegendentry{RadixSeq}
            \addplot table[x=number_of_elements, y=MoellerCompress] {data/tpe-#1-#2.dat};
            \Addlegendentry{MoellerCompress}
            \addplot table[x=number_of_elements, y=RadixSIMD] {data/tpe-#1-#2.dat};
            \Addlegendentry{RadixSIMD}
            \addplot table[x=number_of_elements, y=IPPRadix] {data/tpe-#1-#2.dat};
            \Addlegendentry{IPPRadix}
            \addplot table[x=number_of_elements, y=STLSort] {data/tpe-#1-#2.dat};
            \Addlegendentry{STLSort}
            \addplot table[x=number_of_elements, y=RadixSIMDBramSmall] {data/tpe-#1-#2.dat};
            \Addlegendentry{RadixSIMDBramSmall}
            \addplot table[x=number_of_elements, y=BramasSort] {data/tpe-#1-#2.dat};
            \Addlegendentry{BramasSort}
            \addplot table[x=number_of_elements, y=BlacherSort] {data/tpe-#1-#2.dat};
            \Addlegendentry{BlacherSort}
            \addplot table[x=number_of_elements, y=RadixSIMDNoCmp] {data/tpe-#1-#2.dat};
            \Addlegendentry{RadixSIMDNoCmp}
            %\addplot table[x=number_of_elements, y=RadixSIMDOneReg] {data/tpe-#1.dat};
            %\Addlegendentry{RadixSIMDOneReg}
        \end{axis}
    \end{tikzpicture}
    \caption{Runtime of different algorithms with respect to the number of elements
    for the combination \texttt{#1} with distribution #2.}
    \label{fig:tpe-#1-#2}
\end{figure}
}



\newcommand{\tpeBar}[3]{
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \begin{axis}[xbar,
          xlabel={time per element [ns]},
          width=0.65\textwidth,
          xminorgrids=true,
          xmajorgrids=true,
          xmin=0,
          %xmax=20,
          visualization depends on={rawx \as \rawx},
          nodes near coords={\pgfmathprintnumber\rawx},
          restrict x to domain*={\pgfkeysvalueof{/pgfplots/xmin}:\pgfkeysvalueof{/pgfplots/xmax}},
          nodes near coords align=horizontal,
          point meta=rawx,
          ytick={0,1,2,3,4,5,6,7,8,9},
          yticklabels from table={data/#2-#1-262144.dat}{sort_method},
          bar shift={0pt},
        ]
        \addplot+[
          discard if={sort_method}{RadixSIMDNoCmp},
          discard if={sort_method}{RadixSIMD},
          discard if={sort_method}{RadixSIMDBramSmall},
          discard if={sort_method}{RadixSeq},
          discard if={sort_method}{MoellerSeq},
          discard if={sort_method}{STLSort},
        ] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[discard if not={sort_method}{RadixSIMD}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[discard if not={sort_method}{RadixSIMDBramSmall}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[black,fill=gray,mark=none,discard if not={sort_method}{RadixSeq}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[black,fill=gray,mark=none,discard if not={sort_method}{MoellerSeq}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[black,fill=gray,mark=none,discard if not={sort_method}{STLSort}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[blue!70!white,fill=blue!10!white,dashed, discard if not={sort_method}{RadixSIMDNoCmp}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
      \end{axis}
    \end{tikzpicture}
    \caption{Comparison of the runtime of different algorithms for $2^{18}$ elements with combination \texttt{#2} and distribution #1}
    \label{fig:tpeBar-#1-#2#3}
  \end{figure}
}
\newcommand{\tpeBarxmax}[4]{
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \begin{axis}[xbar,
          xlabel={time per element [ns]},
          width=0.65\textwidth,
          xminorgrids=true,
          xmajorgrids=true,
          xmin=0,
          xmax=#4,
          visualization depends on={rawx \as \rawx},
          nodes near coords={\pgfmathprintnumber\rawx},
          restrict x to domain*={\pgfkeysvalueof{/pgfplots/xmin}:\pgfkeysvalueof{/pgfplots/xmax}},
          nodes near coords align=horizontal,
          point meta=rawx,
          ytick={0,1,2,3,4,5,6,7,8,9},
          yticklabels from table={data/#2-#1-262144.dat}{sort_method},
          bar shift={0pt},
        ]
        \addplot+[
          discard if={sort_method}{RadixSIMDNoCmp},
          discard if={sort_method}{RadixSIMD},
          discard if={sort_method}{RadixSIMDBramSmall},
          discard if={sort_method}{RadixSeq},
          discard if={sort_method}{MoellerSeq},
          discard if={sort_method}{STLSort},
        ] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[discard if not={sort_method}{RadixSIMD}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[discard if not={sort_method}{RadixSIMDBramSmall}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[black,fill=gray,mark=none,discard if not={sort_method}{RadixSeq}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[black,fill=gray,mark=none,discard if not={sort_method}{MoellerSeq}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[black,fill=gray,mark=none,discard if not={sort_method}{STLSort}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
        \addplot+[blue!70!white,fill=blue!10!white,dashed, discard if not={sort_method}{RadixSIMDNoCmp}] table[y expr=\coordindex, x=nanoseconds_per_element] {data/#2-#1-262144.dat};
      \end{axis}
    \end{tikzpicture}
    \caption{Comparison of the runtime of different algorithms for $2^{18}$ elements with combination \texttt{#2} and distribution #1}
    \label{fig:tpeBar-#1-#2#3}
  \end{figure}
}

\widowpenalty=10000
\clubpenalty=10000

\renewcommand{\lstlistingname}{Programmauszug}

\newcommand{\fref}[1]{Abb.~\ref{#1}}
\newcommand{\lfref}[1]{Abbildung~\ref{#1}}
\newcommand{\tref}[1]{Tab.~\ref{#1}}
\newcommand{\ltref}[1]{Tabelle~\ref{#1}}
\newcommand{\sref}[1]{Abschn.~\ref{#1}}
\newcommand{\lsref}[1]{Abschnitt~\ref{#1}}
\newcommand{\cref}[1]{Kap.~\ref{#1}}
\newcommand{\lcref}[1]{Kapitel~\ref{#1}}
\newcommand{\lcsref}[2]{Kapitel~\ref{#1} und \ref{#2}}
\newcommand{\aref}[1]{Anh.~\ref{#1}}
\newcommand{\laref}[1]{Anhang~\ref{#1}}
\newcommand{\lref}[1]{Progr.-ausz.~\ref{#1}}
\newcommand{\lssref}[2]{Progr.-ausz.~\ref{#1} und \ref{#2}}
\newcommand{\llref}[1]{\lstlistingname~\ref{#1}}
\renewcommand{\lineref}[1]{Zeile~\ref{#1}}
\newcommand{\linesref}[2]{Zeilen~\ref{#1} und \ref{#2}}
\newcommand{\linessref}[2]{Zeilen~\ref{#1} bis \ref{#2}}
\newcommand{\llineref}[2]{\lref{#1}, \lineref{#2}}
\newcommand{\llinesref}[3]{\lref{#1}, \linesref{#2}{#3}}
\newcommand{\llinessref}[3]{\lref{#1}, \linessref{#2}{#3}}
\newcommand{\lllineref}[2]{\llref{#1}, \lineref{#2}}
\newcommand{\lllinesref}[3]{\llref{#1}, \linesref{#2}{#3}}
\newcommand{\lllinessref}[3]{\llref{#1}, \linessref{#2}{#3}}
\newcommand{\eref}[1]{Gl.~\ref{#1}}
\newcommand{\leref}[1]{Gleichung~\ref{#1}}
\newcommand{\esref}[2]{Gln.~\ref{#1} und \ref{#2}}
\newcommand{\lesref}[2]{Gleichungen~\ref{#1} und \ref{#2}}
\newcommand{\essref}[2]{Gln.~\ref{#1} bis \ref{#2}}
\newcommand{\lessref}[2]{Gleichungen~\ref{#1} bis \ref{#2}}

\newcommand*{\LastAccess}{Letzter Zugriff: \today}
\newcommand*{\lastAccess}{letzter Zugriff: \today}

\newcommand{\Cpp}{C\nolinebreak[4]\hspace{-.05em}\raisebox{.2ex}{\small{++}}}
\newcommand{\csttexttilde}{\raisebox{0.17ex}{\mbox{$\scriptstyle\mathtt{\sim}$}}}

\addto\captionsngerman{\renewcommand{\listfigurename}{Abbildungsverzeichnis}}
\addto\captionsngerman{\renewcommand{\listtablename}{Tabellenverzeichnis}}
\addto\captionsngerman{\renewcommand{\lstlistlistingname}{\lstlistingname{}sverzeichnis}}
\addto\captionsngerman{\renewcommand{\bibname}{Literaturverzeichnis}}

\arbeit{Bachelor-Arbeit}

\title{SIMD Implementation of MSB Radix Sort with Separate Key and Payload Datastreams}
\author{Jonas Richard Keller}
\betreuera{Prof. Dr.-Ing. Ralf Möller, AG Technische Informatik}
\betreuerb{M.Sc. Jan O'Sullivan, AG Technische Informatik}

\studiengang{Informatik}

\makeindex
\makeglossary{}

\hyphenation{data-streams}

\begin{document}
\raggedbottom{}
%\flushbottom
%\layout
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Titlepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  In this thesis a SIMD (Single Instruction Multiple Data) implementation of the
  MSB Radix Sort algorithm
  with separate key and payload datastreams is presented. The
  implementation makes use of AVX-512 \texttt{mask\_compresstoreu} instructions
  and is based on the implementation with combined key and payload datastreams
  developed by \citet{moeller_radix}.

  This thesis explores whether a separation
  of key and payload datastreams provides an improvement in the
  performance of the SIMD implementation of MSB Radix Sort algorithm.
  The separation
  of key and payload datastreams also enables the algorithm to be generic,
  supporting arbitrary key and payload data types of arbitrary size with an arbitrary
  number of payloads.

  Besides the separation of the key and payload datastreams, other
  improvements to the algorithm are implemented as well.

  Additionally, a thorough analysis of the performance of the algorithm and comparison
  with other algorithms for sorting different types and distributions of data
  is performed.

  The experiments show a significant improvement over
  the implementation by \citet{moeller_radix} for most distributions of data
  up to a factor of 2.
  A portion of the speedup for datasets with a payload is shown to
  be due to the separation of the key and payload datastreams.
  The presented algorithm is however slower than other SIMD implementations
  of sorting algorithms for many distributions of data.
\end{abstract}
\cleardoublepageempty
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Comments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\thispagestyle{empty}
%\vspace*{\fill} % since \vfill does not work
%\noindent{}Diese Seite bietet Platz für Kommentare. TODO\\[\baselineskip]
%\cleardoublepageempty
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\erklaerung{Jonas Keller}{14. Juli 2022}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  toc
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{Roman}
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{chap:introduction}
\newcounter{romanpage}
\setcounter{romanpage}{\value{page}}
\pagenumbering{arabic}

Sorting is a very well-known and fundamental problem in computer
science. Sorting algorithms are used to solve many problems which include
enabling binary search, finding duplicates or finding the k-th largest element.
Thus, sorting algorithms are an essential part of many algorithms
\citetext{\citealp[p. 148]{intro_to_algorithms};
  \citealp[pp. 104-106]{alg_design_manual}}.
%Thus, sorting algorithms are ubiquitous in many applications.

Because of the wide range of applications where sorting is used, the speed of
sorting algorithms is important.

There are many ways to improve the speed of an algorithm.
Some do not require any changes to the algorithm itself, such as higher
CPU clock frequencies or architectural improvements of CPUs.
Other improvements do require changes to the sorting algorithm itself, such as
the use of multiple CPU cores or theoretical improvements
(such as using an algorithm with better complexity).

Another way to speed up some algorithms is to use vector instructions.
With vector instructions, operations can be executed for all elements of a
vector in parallel, instead of just one element at a time.

Some sorting algorithms have already been improved using SIMD instructions.
\citet{moeller_radix} has implemented MSB Radix Sort using AVX-512
\texttt{mask\_compressstoreu} instructions.
However, this implementation only supports datasets with one key and
at most one payload which has to have the same size as the key.
The payloads are also handled in a way that decreases the speedup compared
to key-only sorting significantly.

This thesis aims to improve upon \citeauthor{moeller_radix}'s implementation by separating the key and
payload datastreams and supporting any number of payloads whose sizes
can be different from the key.

The source code of the algorithm developed in this thesis is available
at \url{https://github.com/jonicho/simd-radix-sort} and is licensed under the
MIT license.
This repository contains the algorithm developed in this thesis as a
header-only library \texttt{radixSort.hpp}.
It also contains the source code of the programs used to test the algorithm
and to generate the data sets used for the results.



\chapter{Vectorization using SIMD instructions}\label{chap:simd_instructions}

Since the algorithm developed in this thesis makes use of SIMD instructions,
this chapter provides a short introduction to SIMD instructions.

The abbreviation SIMD stands for \emph{Single Instruction Multiple Data} and is part
of Flynn's taxonomy. Flynn's taxonomy is a classification proposed by \citet{flynn},
which defines the classes SISD (Single Instruction Single Data),
SIMD (Single Instruction Multiple Data), MISD (Multiple Instruction Single Data)
and MIMD (Multiple Instruction Multiple Data).

SIMD (or vector) instructions operate on (SIMD) vectors, which can be thought of as fixed
length arrays of elements of the same length. Usually these vectors are
loaded into SIMD (vector) registers. For example, a SIMD register with a length
of 512 bit might contain 8 elements of 64 bits each.
SIMD instructions operate on these (SIMD) vectors and perform an operation on
each element of the vectors (or datastreams) simultaneously
(hence the name \emph{Single Instruction Multiple Data}).
This is in contrast to conventional
SISD (Single Instruction Single Data) instructions, which apply an operation
to only a single element (or datastream) at a time.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[yscale=0.75]
    \node[anchor=east] at (-0.5,0.5) {vector $a$:};
    \draw (0,0) -- (0,1) -- (1.5,1) -- (1.5,0) -- cycle;
    \node[anchor=mid] at (0.75,0.5) {$a_0$};
    \draw (1.5,1) -- (3,1) -- (3,0) -- (1.5,0);
    \node[anchor=mid] at (2.25,0.5) {$a_1$};
    \draw (3,1) -- (4.5,1) -- (4.5,0) -- (3,0);
    \node[anchor=mid] at (3.75,0.5) {$a_2$};
    \draw (4.5,1) -- (6,1) -- (6,0) -- (4.5,0);
    \node[anchor=mid] at (5.25,0.5) {$a_3$};

    \node[anchor=mid] at (3,-0.5) {$+$};

    \node[anchor=east] at (-0.5,-1.5) {vector $b$:};
    \draw (0, -2) -- (0,-1) -- (1.5,-1) -- (1.5,-2) -- cycle;
    \node[anchor=mid] at (0.75,-1.5) {$b_0$};
    \draw (1.5,-1) -- (3,-1) -- (3,-2) -- (1.5,-2);
    \node[anchor=mid] at (2.25,-1.5) {$b_1$};
    \draw (3,-1) -- (4.5,-1) -- (4.5,-2) -- (3,-2);
    \node[anchor=mid] at (3.75,-1.5) {$b_2$};
    \draw (4.5,-1) -- (6,-1) -- (6,-2) -- (4.5,-2);
    \node[anchor=mid] at (5.25,-1.5) {$b_3$};

    \node[anchor=mid] at (3,-2.5) {$=$};

    \node[anchor=east] at (-0.5,-3.5) {result vector:};
    \draw (0,-4) -- (0,-3) -- (1.5,-3) -- (1.5,-4) -- cycle;
    \node[anchor=mid] at (0.75,-3.5) {$a_0+b_0$};
    \draw (1.5,-3) -- (3,-3) -- (3,-4) -- (1.5,-4);
    \node[anchor=mid] at (2.25,-3.5) {$a_1+b_1$};
    \draw (3,-3) -- (4.5,-3) -- (4.5,-4) -- (3,-4);
    \node[anchor=mid] at (3.75,-3.5) {$a_2+b_2$};
    \draw (4.5,-3) -- (6,-3) -- (6,-4) -- (4.5,-4);
    \node[anchor=mid] at (5.25,-3.5) {$a_3+b_3$};
  \end{tikzpicture}
  \caption{Addition of two 4-element vectors using a SIMD add instruction}
  \label{fig:simd_add_instruction}
\end{figure}

Figure~\ref{fig:simd_add_instruction} shows a vector addition of two 4-element
vectors using a SIMD add instruction as an example. The same operation
(in this case an addition) is performed on each element of the vectors
simultaneously.

The idea of computing using vector instructions is more than 50 years old
with the first computer to feature SIMD instructions being the ILLIAC IV, which was
a supercomputer completed in 1966 \citep{illiac_iv}.

Today, SIMD instructions are featured in most modern CPUs.

\section{SIMD instructions by Intel}

The first SIMD instruction set that Intel introduced for its processors
was the MMX instruction set \citep{washingtonpost_mmx}. It features eight 64 bit
wide SIMD registers (\texttt{mm0} through \texttt{mm7}) which however
re-used floating-point registers and only operated on integers
\citep{enwiki:mmx}. This prevented programs from using floating-point numbers
and MMX instructions at the same time.

To fix these shortcomings, Intel introduced the SSE (Streaming SIMD Extensions)
instruction set in 1999, which later was expanded
to SSE2, SSE3, SSSE3 and SSE4. The SSE instruction sets
introduced 16 independent 128 bit wide SIMD registers
\texttt{xmm0} through \texttt{xmm15} and operations for these registers
which included floating-point operations
\citep{enwiki:sse}.

In 2008, Intel proposed the AVX (Advanced Vector Extensions) instruction set,
which, among other things, expands the SSE registers to 256 bits, calling
them \texttt{ymm0} through \texttt{ymm15}.
Later, AVX was extended to AVX2,
which introduced further new instructions \citep{enwiki:avx}.

\subsection{AVX-512}

AVX-512 is the newest vector instruction set proposed by Intel.
It was introduced in 2013 as the successor to AVX and AVX2 \citep{intel_avx_512}.

The AVX-512 instruction set expands the SSE and AVX vector registers even more
to 512 bits and also doubles the number of vector registers. The AVX-512
registers are called \texttt{zmm0} through \texttt{zmm31} where the bottom
256 bits of each register
are called \texttt{ymm0} to \texttt{ymm31} and the bottom 128 bits of each
register are called \texttt{xmm0} to \texttt{xmm31}.

AVX-512 also introduced new mask registers (\texttt{k0} through \texttt{k7}),
which can be used to mask operations on the vector registers. For example
when adding two vectors, a mask can be used to set certain elements of the
result vector to zero instead of the sum.

AVX-512 actually consists of several instruction sets. The most
important of these is AVX-512F, the foundation, which every CPU supporting
AVX-512 must implement. The AVX-512 instruction sets relevant for this
thesis are

\begin{itemize}
  \item AVX-512BW (Byte and Word Instructions), which adds instructions for
        8-bit and 16-bit integer operations,

  \item AVX512DQ (Doubleword and Quadword Instructions), which adds additional
        32-bit and 64-bit instructions,

  \item AVX-512VL (Vector Length Extensions), which extends most instructions
        to also operate on \texttt{xmm} (128-bit) and \texttt{ymm} (256-bit) registers
        and

  \item AVX512VBMI2 (Vector Byte Manipulation Instructions 2) which adds
        additional instructions for loading and storing 8-bit and 16-bit wide elements.
\end{itemize}

\subsection{Vector intrinsics}

To use SIMD instructions in higher level languages like C, C++ or Rust, for example,
one could use the assembly instructions directly with the use of inline assembly.
However, this defeats the purpose of higher level languages as it is difficult
to write, difficult to maintain, error-prone and
might prevent the compiler from optimizing the code as it does not know
what the inline assembly code does \citep{gcc-wiki-dont-use-inline-assembly}.

For this reason, many high level languages
provide vector intrinsics. These are pseudo-functions that provide the
convenience and type safety of regular functions but are replaced by SIMD
instructions by the compiler and thus do not have any calling overhead.

To use Intel vector intrinsics in C or C++, the file \verb!immintrin.h! must
be included with \texttt{\#include <immintrin.h>}.
This file provides the data types \texttt{\_\_m128}, \texttt{\_\_m256}, \texttt{\_\_m512},
\texttt{\_\_m128d}, \texttt{\_\_m256d}, \texttt{\_\_m512d},
\texttt{\_\_m128i}, \texttt{\_\_m256i} and \texttt{\_\_m512i}.
The numbers in the type names indicate the number of bits the vector can hold,
while the suffix indicates the type of the elements contained in that vector.
No suffix stands for single-precision floating-point values, the suffix
\texttt{d} stands for double-precision floating-point values, and the suffix
\texttt{i} stands for integer values
\citep[pp. 3-12 - 3-14]{intel-manual}.

The intrinsic functions that file provides are prefixed with \texttt{\_mm\_},
\texttt{\_mm256\_} or \texttt{\_mm512\_}, indicating that the function
operates on a vector of 128, 256 or 512 bits, respectively. Additionally,
the functions are suffixed with an indicator of the type of the vector
element the functions operate on. For example, the intrinsic
\texttt{\_mm256\_add\_ps} which has the signature
  {\par\centering\mintinline{c++}{__m256 _mm256_add_ps (__m256 a, __m256 b)}\par}
\noindent adds two 256 bits wide single-precision floating-point
vectors element-wise and returns the resulting vector
\citep[pp. 3-12 - 3-14]{intel-manual}.

Additionally, the file \verb!immintrin.h! provides types for the AVX-512
mask registers, which are called \texttt{\_\_mmask8}, \texttt{\_\_mmask16},
\texttt{\_\_mmask32} and \texttt{\_\_mmask64}, where the number indicates the
number of bits the mask can hold.
Functions on these registers
are prefixed with \texttt{\_k} and suffixed with \texttt{\_mask8},
\texttt{\_mask16}, \texttt{\_mask32} or \texttt{\_mask64}, indicating
that the function operates on a mask of 8, 16, 32 or 64 bits, respectively
\citep{intel_intrinsics_guide}.

\subsection{SSE/AVX* instructions relevant for this thesis}

The following instruction families are relevant for this thesis
\citep{intel_intrinsics_guide}:

\subsection*{\texttt{set1}}
Sets all elements of a vector to a given value.\\
Example intrinsic: \texttt{\_mm512\_set1\_ps}

\subsection*{\texttt{loadu}}
Loads all elements of a vector from memory.
In contrast to the \texttt{load} instruction, the memory address passed to
\texttt{loadu} does not have to be aligned to any particular boundary.\\
Example intrinsic: \texttt{\_mm512\_loadu\_ps}

\subsection*{\texttt{maskz\_loadu}}
Loads all elements of a vector
from memory, but only those elements where the corresponding bit in a given
mask is set to 1. If the bit in the mask is set to 0, the element is set to 0.\\
Example intrinsic: \texttt{\_mm512\_maskz\_loadu\_ps}\\
Figure~\ref{fig:maskz_loadu_example} shows an example for the \texttt{maskz\_loadu}
instruction.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[yscale=0.625, xscale=0.625]
    \node[anchor=east] at (-0.5,0.5) {mask};
    \node[anchor=mid] at (-0.8,1.5) {\small bit};
    \draw (0,1) -- (0,1.2);
    \node[anchor=mid] at (0,1.5) {0};
    \draw (0,0) -- (0,1) -- (2,1) -- (2,0) -- cycle;
    \node[anchor=mid] at (1,0.5) {1};
    \draw (2,1) -- (2,1.2);
    \node[anchor=mid] at (2,1.5) {1};
    \draw (2,1) -- (4,1) -- (4,0) -- (2,0);
    \node[anchor=mid] at (3,0.5) {0};
    \draw (4,1) -- (4,1.2);
    \node[anchor=mid] at (4,1.5) {2};
    \draw (4,1) -- (6,1) -- (6,0) -- (4,0);
    \node[anchor=mid] at (5,0.5) {0};
    \draw (6,1) -- (6,1.2);
    \node[anchor=mid] at (6,1.5) {3};
    \draw (6,1) -- (8,1) -- (8,0) -- (6,0);
    \node[anchor=mid] at (7,0.5) {1};
    \draw (8,1) -- (8,1.2);
    \node[anchor=mid] at (8,1.5) {4};
    \draw (8,1) -- (10,1) -- (10,0) -- (8,0);
    \node[anchor=mid] at (9,0.5) {1};
    \draw (10,1) -- (10,1.2);
    \node[anchor=mid] at (10,1.5) {5};
    \draw (10,1) -- (12,1) -- (12,0) -- (10,0);
    \node[anchor=mid] at (11,0.5) {0};
    \draw (12,1) -- (12,1.2);
    \node[anchor=mid] at (12,1.5) {6};
    \draw (12,1) -- (14,1) -- (14,0) -- (12,0);
    \node[anchor=mid] at (13,0.5) {1};
    \draw (14,1) -- (14,1.2);
    \node[anchor=mid] at (14,1.5) {7};
    \draw (14,1) -- (16,1) -- (16,0) -- (14,0);
    \node[anchor=mid] at (15,0.5) {0};
    \draw (16,1) -- (16,1.2);
    \node[anchor=mid] at (16,1.5) {8};

    \node[anchor=east] at (-0.5,-2) {memory};
    \node[anchor=east] at (-0.8,-0.8) {\small \shortstack{byte offset\\from address}};
    \draw[dashed] (0,-2.5) -- (-1,-2.5);
    \draw[dashed] (0,-1.5) -- (-1,-1.5);
    \draw (0,-1.5) -- (0,-1.3);
    \node[anchor=mid] at (0,-1) {+0};
    \draw (0,-2.5) -- (0,-1.5) -- (2,-1.5) -- (2,-2.5) -- cycle;
    \node[anchor=mid] at (1,-2) {140};
    \draw (2,-1.5) -- (2,-1.3);
    \node[anchor=mid] at (2,-1) {+1};
    \draw (2,-2.5) -- (4,-2.5) -- (4,-1.5) -- (2,-1.5);
    \node[anchor=mid] at (3,-2) {112};
    \draw (4,-1.5) -- (4,-1.3);
    \node[anchor=mid] at (4,-1) {+2};
    \draw (4,-2.5) -- (6,-2.5) -- (6,-1.5) -- (4,-1.5);
    \node[anchor=mid] at (5,-2) {247};
    \draw (6,-1.5) -- (6,-1.3);
    \node[anchor=mid] at (6,-1) {+3};
    \draw (6,-2.5) -- (8,-2.5) -- (8,-1.5) -- (6,-1.5);
    \node[anchor=mid] at (7,-2) {88};
    \draw (8,-1.5) -- (8,-1.3);
    \node[anchor=mid] at (8,-1) {+4};
    \draw (8,-2.5) -- (10,-2.5) -- (10,-1.5) -- (8,-1.5);
    \node[anchor=mid] at (9,-2) {76};
    \draw (10,-1.5) -- (10,-1.3);
    \node[anchor=mid] at (10,-1) {+5};
    \draw (10,-2.5) -- (12,-2.5) -- (12,-1.5) -- (10,-1.5);
    \node[anchor=mid] at (11,-2) {206};
    \draw (12,-1.5) -- (12,-1.3);
    \node[anchor=mid] at (12,-1) {+6};
    \draw (12,-2.5) -- (14,-2.5) -- (14,-1.5) -- (12,-1.5);
    \node[anchor=mid] at (13,-2) {175};
    \draw (14,-1.5) -- (14,-1.3);
    \node[anchor=mid] at (14,-1) {+7};
    \draw (14,-2.5) -- (16,-2.5) -- (16,-1.5) -- (14,-1.5);
    \node[anchor=mid] at (15,-2) {21};
    \draw (16,-1.5) -- (16,-1.3);
    \node[anchor=mid] at (16,-1) {+8};
    \draw[dashed] (16,-2.5) -- (17,-2.5);
    \draw[dashed] (16,-1.5) -- (17,-1.5);

    \node[anchor=east] at (-0.5,-4.5) {result vector};
    \node[anchor=mid] at (-0.8,-5.6) {\small bit};
    \draw (0,-5) -- (0,-5.2);
    \node[anchor=mid] at (0,-5.6) {0};
    \draw (0,-5) -- (0,-4) -- (2,-4) -- (2,-5) -- cycle;
    \node[anchor=mid] at (1,-4.5) {140};
    \draw (2,-5) -- (2,-5.2);
    \node[anchor=mid] at (2,-5.6) {8};
    \draw (2,-5) -- (4,-5) -- (4,-4) -- (2,-4);
    \node[anchor=mid] at (3,-4.5) {0};
    \draw (4,-5) -- (4,-5.2);
    \node[anchor=mid] at (4,-5.6) {16};
    \draw (4,-5) -- (6,-5) -- (6,-4) -- (4,-4);
    \node[anchor=mid] at (5,-4.5) {0};
    \draw (6,-5) -- (6,-5.2);
    \node[anchor=mid] at (6,-5.6) {24};
    \draw (6,-5) -- (8,-5) -- (8,-4) -- (6,-4);
    \node[anchor=mid] at (7,-4.5) {88};
    \draw (8,-5) -- (8,-5.2);
    \node[anchor=mid] at (8,-5.6) {32};
    \draw (8,-5) -- (10,-5) -- (10,-4) -- (8,-4);
    \node[anchor=mid] at (9,-4.5) {76};
    \draw (10,-5) -- (10,-5.2);
    \node[anchor=mid] at (10,-5.6) {40};
    \draw (10,-5) -- (12,-5) -- (12,-4) -- (10,-4);
    \node[anchor=mid] at (11,-4.5) {0};
    \draw (12,-5) -- (12,-5.2);
    \node[anchor=mid] at (12,-5.6) {48};
    \draw (12,-5) -- (14,-5) -- (14,-4) -- (12,-4);
    \node[anchor=mid] at (13,-4.5) {175};
    \draw (14,-5) -- (14,-5.2);
    \node[anchor=mid] at (14,-5.6) {56};
    \draw (14,-5) -- (16,-5) -- (16,-4) -- (14,-4);
    \node[anchor=mid] at (15,-4.5) {0};
    \draw (16,-5) -- (16,-5.2);
    \node[anchor=mid] at (16,-5.6) {64};

    \draw[-latex] (1,-2.5) -- (1,-4);
    \draw[-latex] (7,-2.5) -- (7,-4);
    \draw[-latex] (9,-2.5) -- (9,-4);
    \draw[-latex] (13,-2.5) -- (13,-4);
  \end{tikzpicture}
  \caption{Example for the \texttt{maskz\_loadu} instruction.
    A vector of 8 8-bit integers is loaded from memory into a vector register using the
    \texttt{maskz\_loadu} instruction with an 8-bit mask.}
  \label{fig:maskz_loadu_example}
\end{figure}

\subsection*{\texttt{mask\_compressstoreu}}
Stores all elements of a vector
where the corresponding bit in a given mask is set to 1 contiguously in memory.\\
Example intrinsic: \texttt{\_mm512\_mask\_compressstoreu\_ps}\\
Figure~\ref{fig:compressstoreu_example} shows an example for the
\texttt{mask\_compressstoreu} instruction.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[yscale=0.625, xscale=0.625]
    \node[anchor=east] at (-0.5,0.5) {mask};
    \node[anchor=mid] at (-0.8,1.5) {\small bit};
    \draw (0,1) -- (0,1.2);
    \node[anchor=mid] at (0,1.5) {0};
    \draw (0,0) -- (0,1) -- (2,1) -- (2,0) -- cycle;
    \node[anchor=mid] at (1,0.5) {1};
    \draw (2,1) -- (2,1.2);
    \node[anchor=mid] at (2,1.5) {1};
    \draw (2,1) -- (4,1) -- (4,0) -- (2,0);
    \node[anchor=mid] at (3,0.5) {0};
    \draw (4,1) -- (4,1.2);
    \node[anchor=mid] at (4,1.5) {2};
    \draw (4,1) -- (6,1) -- (6,0) -- (4,0);
    \node[anchor=mid] at (5,0.5) {0};
    \draw (6,1) -- (6,1.2);
    \node[anchor=mid] at (6,1.5) {3};
    \draw (6,1) -- (8,1) -- (8,0) -- (6,0);
    \node[anchor=mid] at (7,0.5) {1};
    \draw (8,1) -- (8,1.2);
    \node[anchor=mid] at (8,1.5) {4};
    \draw (8,1) -- (10,1) -- (10,0) -- (8,0);
    \node[anchor=mid] at (9,0.5) {1};
    \draw (10,1) -- (10,1.2);
    \node[anchor=mid] at (10,1.5) {5};
    \draw (10,1) -- (12,1) -- (12,0) -- (10,0);
    \node[anchor=mid] at (11,0.5) {0};
    \draw (12,1) -- (12,1.2);
    \node[anchor=mid] at (12,1.5) {6};
    \draw (12,1) -- (14,1) -- (14,0) -- (12,0);
    \node[anchor=mid] at (13,0.5) {1};
    \draw (14,1) -- (14,1.2);
    \node[anchor=mid] at (14,1.5) {7};
    \draw (14,1) -- (16,1) -- (16,0) -- (14,0);
    \node[anchor=mid] at (15,0.5) {0};
    \draw (16,1) -- (16,1.2);
    \node[anchor=mid] at (16,1.5) {8};

    \node[anchor=east] at (-0.5,-2) {vector};
    \node[anchor=mid] at (-0.8,-1) {\small bit};
    \draw (0,-1.5) -- (0,-1.3);
    \node[anchor=mid] at (0,-1) {0};
    \draw (0,-2.5) -- (0,-1.5) -- (2,-1.5) -- (2,-2.5) -- cycle;
    \node[anchor=mid] at (1,-2) {140};
    \draw (2,-1.5) -- (2,-1.3);
    \node[anchor=mid] at (2,-1) {8};
    \draw (2,-2.5) -- (4,-2.5) -- (4,-1.5) -- (2,-1.5);
    \node[anchor=mid] at (3,-2) {112};
    \draw (4,-1.5) -- (4,-1.3);
    \node[anchor=mid] at (4,-1) {16};
    \draw (4,-2.5) -- (6,-2.5) -- (6,-1.5) -- (4,-1.5);
    \node[anchor=mid] at (5,-2) {247};
    \draw (6,-1.5) -- (6,-1.3);
    \node[anchor=mid] at (6,-1) {24};
    \draw (6,-2.5) -- (8,-2.5) -- (8,-1.5) -- (6,-1.5);
    \node[anchor=mid] at (7,-2) {88};
    \draw (8,-1.5) -- (8,-1.3);
    \node[anchor=mid] at (8,-1) {32};
    \draw (8,-2.5) -- (10,-2.5) -- (10,-1.5) -- (8,-1.5);
    \node[anchor=mid] at (9,-2) {76};
    \draw (10,-1.5) -- (10,-1.3);
    \node[anchor=mid] at (10,-1) {40};
    \draw (10,-2.5) -- (12,-2.5) -- (12,-1.5) -- (10,-1.5);
    \node[anchor=mid] at (11,-2) {206};
    \draw (12,-1.5) -- (12,-1.3);
    \node[anchor=mid] at (12,-1) {48};
    \draw (12,-2.5) -- (14,-2.5) -- (14,-1.5) -- (12,-1.5);
    \node[anchor=mid] at (13,-2) {175};
    \draw (14,-1.5) -- (14,-1.3);
    \node[anchor=mid] at (14,-1) {56};
    \draw (14,-2.5) -- (16,-2.5) -- (16,-1.5) -- (14,-1.5);
    \node[anchor=mid] at (15,-2) {21};
    \draw (16,-1.5) -- (16,-1.3);
    \node[anchor=mid] at (16,-1) {64};

    \node[anchor=east] at (-0.5,-4.5) {memory};
    \node[anchor=east] at (-0.8,-5.8) {\small \shortstack{byte offset\\from address}};
    \draw (0,-5) -- (0,-5.2);
    \node[anchor=mid] at (0,-5.6) {+0};
    \draw[dashed] (0,-5) -- (-1,-5);
    \draw[dashed] (0,-4) -- (-1,-4);
    \draw (0,-5) -- (0,-4) -- (2,-4) -- (2,-5) -- cycle;
    \node[anchor=mid] at (1,-4.5) {140};
    \draw (2,-5) -- (2,-5.2);
    \node[anchor=mid] at (2,-5.6) {+1};
    \draw (2,-5) -- (4,-5) -- (4,-4) -- (2,-4);
    \node[anchor=mid] at (3,-4.5) {88};
    \draw (4,-5) -- (4,-5.2);
    \node[anchor=mid] at (4,-5.6) {+2};
    \draw (4,-5) -- (6,-5) -- (6,-4) -- (4,-4);
    \node[anchor=mid] at (5,-4.5) {76};
    \draw (6,-5) -- (6,-5.2);
    \node[anchor=mid] at (6,-5.6) {+3};
    \draw (6,-5) -- (8,-5) -- (8,-4) -- (6,-4);
    \node[anchor=mid] at (7,-4.5) {175};
    \draw (8,-5) -- (8,-5.2);
    \node[anchor=mid] at (8,-5.6) {+4};
    \draw (8,-5) -- (10,-5) -- (10,-4) -- (8,-4);
    \node[anchor=mid] at (9,-4.5) {};
    \draw (10,-5) -- (10,-5.2);
    \node[anchor=mid] at (10,-5.6) {+5};
    \draw (10,-5) -- (12,-5) -- (12,-4) -- (10,-4);
    \node[anchor=mid] at (11,-4.5) {};
    \draw (12,-5) -- (12,-5.2);
    \node[anchor=mid] at (12,-5.6) {+6};
    \draw (12,-5) -- (14,-5) -- (14,-4) -- (12,-4);
    \node[anchor=mid] at (13,-4.5) {};
    \draw (14,-5) -- (14,-5.2);
    \node[anchor=mid] at (14,-5.6) {+7};
    \draw (14,-5) -- (16,-5) -- (16,-4) -- (14,-4);
    \node[anchor=mid] at (15,-4.5) {};
    \draw (16,-5) -- (16,-5.2);
    \node[anchor=mid] at (16,-5.6) {+8};
    \draw[dashed] (16,-5) -- (17,-5);
    \draw[dashed] (16,-4) -- (17,-4);

    \draw[-latex] (1,-2.5) -- (1,-4);
    \draw[-latex] (7,-2.5) -- (3,-4);
    \draw[-latex] (9,-2.5) -- (5,-4);
    \draw[-latex] (13,-2.5) -- (7,-4);
  \end{tikzpicture}
  \caption{Example for the \texttt{mask\_compressstoreu} instruction.
    A vector of 8 8-bit integers is stored into memory using the
    \texttt{mask\_compressstoreu} instruction with an 8-bit mask.}
  \label{fig:compressstoreu_example}
\end{figure}

\subsection*{\texttt{test\_mask}}
Computes the bitwise AND of two
vectors, and returns a mask where a bit is set if the corresponding element
in the result vector is non-zero.\\
Example intrinsic: \texttt{\_mm512\_mask\_test\_epi32\_mask}

\subsection*{\texttt{knot}}
Computes the bitwise NOT of a mask.\\
Example intrinsic: \texttt{\_knot\_mask32}

\subsection*{\texttt{kand}}
Computes the bitwise AND of two masks.\\
Example intrinsic: \texttt{\_kand\_mask32}

\subsection*{\texttt{kpopcnt}}
Counts the number of set bits in a mask (\textbf{pop}ulation \textbf{c}ou\textbf{nt}).\\
Strictly speaking, this instruction is not part of the SSE or one of the AVX
instruction sets, but it is needed for the algorithm presented in this thesis,
so it is included in this list anyway.


\chapter{Related Work}

Vectorized sorting algorithms are an extensively researched topic resulting
in vast amounts of literature. Thus only some of the most relevant
research can be presented in this section.

One of the first sorting algorithms designed for parallel computers was
bitonic sort developed by \citet{first_bitonic_sort} which was implemented
using SIMD instructions for supercomputers by \citet{simd_bitonic_sort}.
Bitonic sort is a sorting network which is a type of sorting algorithm
where the order of comparisons is set in advance that sorts a fixed number of
elements \citep{enwiki:sorting_network}.

\citet{radix_cray_y-mp} developed a vectorized radix sort algorithm for the
CRAY Y-MP supercomputer. They achieve a speedup of 3 to 5 over the vectorized
implementation of Quicksort by \citet{levin_vectorized_quicksort}.
The speedup is at least partly due to the fact that the CRAY Y-MP
does not have
a cache which results in all memory being accessible with equal cost.

More recently, \citet{fast_quicksort_gueron_krasnov} implemented a vectorized
Quicksort using AVX2, which switches to insertion sort for small subarrays.
Their partitioning function uses precomputed permutation
masks and requires $\mathcal{O}(n)$
additional memory. They achieved a speedup of 4 compared to STL Sort,
the sorting algorithm included in the C++ standard library
\citep{enwiki:stl_sort}.

Using AVX-512, \citet{bramas} designed a Quicksort which makes use of
AVX-512 \texttt{mask\_compressstoreu} instructions for the Quicksort partitioning.
In contrast to the implementation of \citet{fast_quicksort_gueron_krasnov},
his partitioning does not require any additional
memory or precomputed permutation masks. He achieved this by buffering the
outermost vectors in registers and using the
\texttt{mask\_compressstoreu} instructions.
For small subarrays, his implementation switches to a bitonic sort.

Another Quicksort implementation was developed by \citet{blacher} using
AVX2. For the Quicksort partitioning they used the vector buffering by
\citet{bramas} to avoid the need for additional memory
and the precomputed permutation masks by \citet{fast_quicksort_gueron_krasnov}.
For small subarrays, their implementation uses sorting networks.
\citet{blacher_using_avx512_thiemicke} port this algorithm to AVX-512, which
however is slower than the original implementation for large arrays.

Around the same time, \citet{moeller_radix} proposed a SIMD implementation of
bitwise MSB radix sort using AVX-512 instructions.
He used the AVX-512 \texttt{mask\_compressstoreu}
instructions to implement a vectorized in-place bit sorter algorithm, which
is used to sort an array of elements according to a specific bit.
For small subarrays, his implementation, similar to the implementation by
\citet{fast_quicksort_gueron_krasnov}, switches to insertion sort.
His bit sorter algorithm buffers vectors in SIMD registers to make the algorithm
in-place, similar to the partitioning function by \citet{bramas}.

This thesis builds on the implementation by \citet{moeller_radix}.


\chapter{MSB Radix sort}\label{chap:radix_sort}

The algorithm presented in this thesis uses in-place MSB Radix Sort.
MSB Radix Sort will first be described for sorting unsigned integers into
ascending order and then be generalized to be able to sort signed integers
and floating point numbers as well as being able to sort an array into
descending order.

Unlike many other sorting algorithms, MSB Radix Sort does not sort by comparing keys
with each other. Instead, it sorts an array
by examining the bits of the binary representation of the keys
\citep[pp. 122-123]{knuth_taocp}.

\begin{algorithm}
  \caption{MSB Radix sort}\label{algo:radix_sort}
  %\textbf{Input:} An array $a$ of $n$ $k$-bit unsigned integers.
  \small
  \begin{algorithmic}[1]
    \Procedure{MsbRadixSort}{\texttt{array}, \texttt{length}}
    \State \Call{RadixRecursion}{\texttt{array}, 0, $\texttt{length}-1$, number of most significant bit}
    \EndProcedure
    \Procedure{RadixRecursion}{\texttt{array}, \texttt{left}, \texttt{right}, \texttt{bitNo}}
    \If{$\texttt{right}-\texttt{left} \le 0$} \Return \EndIf
    \State $\texttt{split} \gets$ \Call{BitSort}{\texttt{array}, \texttt{left}, \texttt{right}, \texttt{bitNo}}
    \If{$\texttt{bitNo}-1\ge 0$}
    \State \Call{RadixRecursion}{\texttt{array}, \texttt{left}, $\texttt{split}-1$, $\texttt{bitNo}-1$}
    \State \Call{RadixRecursion}{\texttt{array}, \texttt{split}, \texttt{right}, $\texttt{bitNo}-1$}
    \EndIf
    \EndProcedure
    \Procedure{BitSort}{\texttt{array}, \texttt{left}, \texttt{right}, \texttt{bitNo}}
    \While{\texttt{true}}
    \State $\texttt{i}\gets$ \parbox[t][2\baselineskip]{.8\linewidth}{index of leftmost element between \texttt{left} and \texttt{right} that has bit \texttt{bitNo} set to 1}
    \label{lst:bit_sort_loop_bit_test_left}
    \State $\texttt{j}\gets$ \parbox[t][2\baselineskip]{.8\linewidth}{index of rightmost element between \texttt{left} and \texttt{right} that has bit \texttt{bitNo} set to 0}
    \label{lst:bit_sort_loop_bit_test_right}
    \If{$\texttt{i}\ge \texttt{j}$} \Return \texttt{i} \EndIf
    \State swap $\texttt{array[i]}$ and $\texttt{array[j]}$
    \EndWhile
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Algorithm~\ref{algo:radix_sort} shows the MSB Radix Sort algorithm.
It works in the following way:

First, the array is sorted according to the most significant bit of the keys.
This results in two subarrays, one containing the keys with the most significant
bit set to 0 and one containing the keys with the most significant bit set to 1.
The two subarrays are then recursively sorted according to the next most
significant bit and so on.
The recursion stops when a subarray is empty or all bits were sorted
\citep[pp. 122-128]{knuth_taocp}.

The bit sorter works by finding the leftmost element with the bit set to 1
and the rightmost element with the bit set to 0, i.e. the first elements from the
left and right, which are at the `wrong' location. Those two
elements are swapped so that they are in the `right' position.
This is then repeated until the elements are sorted
by the bit (this can be detected by the fact that the leftmost element
whose bit is set to 1 is right of the rightmost element whose bit is set
to 0). This results in two subarrays, where all of the elements in the first
subarray are smaller than all of the elements in the second subarray
\citep[pp. 122-128]{knuth_taocp}.

%The algorithm works because of the following property of the binary representation of
%unsigned integers:
%
%Let $a,b$ be two $n$-bit unsigned integers and $i\in\{0,\dots,n-1\}$ such that
%$\forall j>i: a_j=b_j$ (where $c_k$ denotes bit number
%$k$ of the $n$-bit unsigned integer $c$). Then $a_i<b_i\Leftrightarrow a<b$.
%
%This property means that, given an array of unsigned integers where the upper
%$k$ bits of all elements are equal ($k$ can be 0), if the array is split
%into two subarrays according to bit $k+1$, then all elements in the first
%subarray are smaller than all elements in the second subarray.
%It then follows inductively that the result of the algorithm is a sorted array.


\section{Sorting different data types}
So far, the MSB Radix Sort algorithm has only been described for sorting unsigned integers
in ascending order.
But, one may also want to sort other types of data or sort data in descending order.
Luckily, the algorithm can very easily be extended to also sort in descending order and
handle other data types,
as long as the bits of the data type are ordered by significance, i.e. the
most significant bit is the highest bit (bit 31 in case of a 32-bit data type)
and the least significant bit is the lowest bit (bit 0).
%(That means for a $n$-bit data type there must exist a permutation $\pi:\{0,\dots,n-1\}\to\{0,\dots,n-1\}$
%such that bit $\pi(i)$ has less significance than bit $\pi(i+1)$ for all $i\in\{0,\dots,n-2\}$).
In this thesis only unsigned integers, signed integers and
IEEE-754 floating point numbers are considered.

\begin{table}[b!]
  \centering
  \begin{tabular}{c|c|c}
    bit index                      & $n-1$ & $n-2,\dots,0$         \\
    \hline
    unsigned integer               & up    & up                    \\
    signed integer                 & down  & up                    \\
    IEEE-754 floating point number & down  & up if bit $n-1$ is 0, \\
                                   &       & down otherwise        \\
  \end{tabular}
  \caption{Bit sort directions for sorting different $n$-bit data types in ascending order
    \citep{float_radix_sort, moeller_radix}}
  \label{tab:bit_sort_directions}
\end{table}

Conveniently, unsigned integers, signed integers and IEEE-754 floating point numbers
have the property that the bits are ordered by significance \citep{float_radix_sort},
thus only the direction the individual bits have to be sorted in differs for these
three data types.
To sort a specific bit in descending instead of ascending order, only
the order of the two tests on line~\ref{lst:bit_sort_loop_bit_test_left}
and \ref{lst:bit_sort_loop_bit_test_right} in algorithm~\ref{algo:radix_sort}
needs to be changed.

Table~\ref{tab:bit_sort_directions} shows the direction the bits of different
data types have to be sorted. Using this table the MSB Radix Sort algorithm can be extended to sort
unsigned integers, signed integers and IEEE-754 floating point numbers.

To sort an array descending instead of ascending, the bit sort directions can simply be
inverted. This inverts the sort direction of every bit and thus the sort direction
of the whole data type.


\section{Comparison sorter for small subarrays}
\label{sec:comparison_sorter_for_small_subarrays_seq}

For smaller subarrays the cost of the recursive function calls
per element increases, since for smaller subarrays less elements are sorted
per call. To combat this, the algorithm uses a comparison sorter for subarrays
with a size below the threshold \texttt{cmpSortThreshold}.
The comparison sorting algorithm used is insertion sort, which despite
its runtime complexity of $\mathcal{O}(n^2)$ is faster than most other sorting algorithms
for small arrays \citep{cs241_sorting}.

The optimal value for \texttt{cmpSortThreshold} is explored in
section~\ref{sec:determining-the-optimal-comparison-sorter-threshold}.



\chapter{C++ implementation}\label{chap:cpp_implementation}

The sorting algorithms presented in this thesis are implemented in C++.
The language was chosen because of its widespread use and because of its
template features, which are used to implement the sorting algorithm in
a generic fashion.

Similar to the MSB Radix Sort implementation by \citet{moeller_radix},
the different versions of MSB Radix Sort implemented for this thesis share
the same recursion function (in this thesis called radix recursion).

\begin{listing}[h]
  \begin{minted}[
    fontsize=\fontsize{9pt}{9pt},
    linenos,
    bgcolor=white,
    numbersep=5pt
    ]{c++}
template <bool Up, typename BitSorter, typename CmpSorter,
          bool IsRightSide = false, bool IsHighestBit = true,
          typename K, typename... Ps>
void radixRecursion(int bitNo, SortIndex cmpSortThreshold, SortIndex left,
                    SortIndex right, K *keys, Ps *...payloads) {
  if (right - left <= 0) { return; }
  if (right - left < cmpSortThreshold) {
    CmpSorter::template sort<Up, K, Ps...>(left, right, keys, payloads...);
    return;
  }
  SortIndex split = BitSorter::template sortBit<Up, IsHighestBit, IsRightSide,
    K, Ps...>(bitNo, left, right, keys, payloads...);
  if (bitNo > 0) {
    radixRecursion
      <Up, BitSorter, CmpSorter, IsHighestBit ? false : IsRightSide, false>
        (bitNo - 1, cmpSortThreshold, left, split - 1, keys, payloads...);
    radixRecursion
      <Up, BitSorter, CmpSorter, IsHighestBit ? true : IsRightSide, false>
        (bitNo - 1, cmpSortThreshold, split, right, keys, payloads...);
  }
}
  \end{minted}
  \caption{C++ implementation of the radix recursion}
  \label{lst:cpp_radix_recursion}
\end{listing}

Listing~\ref{lst:cpp_radix_recursion} shows the C++ implementation of the
radix recursion as a template function. The implementation as a template
function makes the algorithm generic, so that it can be used to sort data sets
with arbitrary key and payload data type combinations and also
provides a means of configuring different bit sorter and comparison
sort algorithms at compile time.

For configuring the bit sorter and comparison sort algorithms, the template
parameters \texttt{BitSorter} and \texttt{CmpSorter} are used.
The different algorithms are passed as classes that contain a template
function \texttt{sort} or \texttt{sortBit}, respectively.
The \texttt{BitSorter} sorts a specific bit
and the \texttt{CmpSorter} sorts a whole subarray
with a comparison based sorting algorithm. The function parameter
\texttt{cmpSortThreshold} controls the threshold for using the comparison
based sorting algorithm, as described in section~\ref{sec:comparison_sorter_for_small_subarrays_seq}.

The next two template parameters \texttt{IsRightSide} and \texttt{IsHighestBit}
are used to keep track whether the algorithm is currently on the root of the
sorting tree (parameter \texttt{IsHighestBit}) and, if not, on which side of the
sorting tree the algorithm is currently working on (parameter \texttt{IsRightSide}).
These two parameters, together with the parameter \texttt{Up}
(which controls the sort direction), are used to
determine the sort direction of the current bit according to
table~\ref{tab:bit_sort_directions} in chapter~\ref{chap:radix_sort}.

The last template parameters \texttt{K} and \texttt{Ps...}
determine the data type of the keys and payloads. Most of the time these
can be automatically deduced by the compiler \citep{cpp_templates}.

\begin{listing}[htb!]
  \begin{minted}[
    fontsize=\fontsize{9pt}{9pt},
    linenos,
    bgcolor=white,
    numbersep=5pt
    ]{c++}
struct BitSorterSequential {
  template <bool Up, bool IsHighestBit, bool IsRightSide,
            typename K, typename... Ps>
  static INLINE SortIndex sort(int bitNo, SortIndex left, SortIndex right,
                               K *keys, Ps *...payloads) {
    SortIndex l = left; SortIndex r = right;
    while (l <= r) {
      while (l <= r && (bitDirUp<K, Up, IsHighestBit, IsRightSide>() !=
                        isBitSet(bitNo, keys[l])))
        { l++; }
      while (l <= r && (!bitDirUp<K, Up, IsHighestBit, IsRightSide>() !=
                        isBitSet(bitNo, keys[r])))
        { r--; }
      if (l < r) {
        std::swap(keys[l], keys[r]); (std::swap(payloads[l], payloads[r]), ...);
      }
    }
    return l;
  }
};
  \end{minted}
  \caption{C++ implementation of the sequential bit sorter}
  \label{lst:cpp_seq_bit_sort}
\end{listing}

Listing~\ref{lst:cpp_seq_bit_sort} shows the C++ implementation of the
sequential bit sorter as described in chapter~\ref{chap:radix_sort}.
This
class can be passed as the \texttt{BitSorter} template parameter to the
radix recursion, to use the sequential bit sorter as the bit sorter algorithm.
The template function \texttt{bitDirUp} used in this class provides an implementation
of table~\ref{tab:bit_sort_directions} for the bit sort direction.

\section{Template-wrapper for SIMD instructions}
\label{sec:template_wrapper}

As explained in chapter~\ref{chap:simd_instructions},
SIMD instructions can be accessed from C++ via vector intrinsics.
The SIMD version of MSB Radix Sort could be implemented using those intrinsics.
However, vector intrinsics are
specific to the vector size and the data type
of the vector elements.
This makes it impractical to develop a generic algorithm that can be used for
arbitrary key and payload data type combinations, since
it would have to be implemented for each key and payload data type combination
separately.

To overcome this, a template wrapper library can be used, which wraps
the vector registers and intrinsics in template
classes and functions. This allows the algorithm to be implemented in a generic
fashion for arbitrary key and payload data type combinations where the appropriate
vector registers and intrinsics are chosen at compile time.

There are such template wrapper libraries available, for example
the T-SIMD library developed by \citet{moeller_tsimd}.
However, the T-SIMD library does not provide support for 64-bit wide data types
(\texttt{uint64\_t}, \texttt{int64\_t} and \texttt{double}) and only provides
16, 32 and 64 byte wide vectors.
%This does make sense, given that the
%AVX* instruction sets only provide 16, 32 and 64 byte wide vector registers.

But the MSB Radix Sort implementation for this thesis requires vectors with
byte widths of 8, 16, 32, 64, 128, 256 and 512 and also for 64-bit wide data
types.



Therefore, for this thesis, a separate template wrapper was implemented based on
the T-SIMD library.
Listing~\ref{lst:cpp_simd_vector} shows the implementation
of the SIMD vector class of that library, where \texttt{MMRegType<T, Bytes>} is
the appropriate vector register type
(i.e. one of \texttt{\_\_128}, \texttt{\_\_m256}, etc.)
for the type \texttt{T} and number of bytes
\texttt{Bytes}. Listing~\ref{lst:cpp_simd_mask} shows the implementation of the SIMD mask
class, where \texttt{MaskType<Size>} is the appropriate mask register type
(i.e. one of \texttt{\_\_mmask64}, \texttt{\_\_mmask32}, etc.).
Listing~\ref{lst:cpp_simd_loadu} shows is the implementation of the \texttt{loadu} instruction
of that library as an example.


Since the AVX* instruction sets only provide 16, 32 and 64 byte wide vector
registers, the vectors of sizes 8, 128, 256 and 512 are emulated.
For the 8 byte wide vectors, a 16 byte vector register is used, with the upper
8 bytes simply being ignored. Operations on vectors of this size use the AVX-512
masked instructions with a mask where the upper 8 bits are set to 0 to
emulate operations on the 8 byte wide vectors, as can be seen on
lines~\ref{line:op_on_8bytes_begin} to \ref{line:op_on_8bytes_end} in
listing~\ref{lst:cpp_simd_loadu}.
To emulate the 128, 256 and 512 byte wide vectors, multiple 64 byte wide
vector registers are combined into a single vector, as shown in
listing~\ref{lst:cpp_simd_vector}.
Operations on these
emulated vectors are then repeated for each of the 64 byte wide vector
registers, emulating the operations on the 128, 256 and 512 byte wide vectors,
as can be seen on lines~\ref{line:op_on_large_begin} to \ref{line:op_on_large_end}
in listing~\ref{lst:cpp_simd_loadu}.



\begin{listing}[h!]
  \begin{minted}[
    fontsize=\fontsize{9pt}{9pt},
    linenos,
    bgcolor=white,
    numbersep=5pt
    ]{c++}
template <typename T, int Bytes = 64, typename = void> struct Vec;

template <typename T, int Bytes>
struct Vec<T, Bytes, std::enable_if_t<(Bytes <= 64) && is_power_of_two<Bytes>>> {
  MMRegType<T, Bytes> mmReg;
  static constexpr int numElems = Bytes / sizeof(T);
  Vec() = default;
  Vec(const MMRegType<T, Bytes> x) : mmReg(x) {}
  Vec &operator=(const MMRegType<T, Bytes> x) { mmReg = x; return *this; }
  operator MMRegType<T, Bytes>() const { return mmReg; }
};

template <typename T, int Bytes>
struct Vec<T, Bytes, std::enable_if_t<(Bytes > 64) && is_power_of_two<Bytes>>> {
  MMRegType<T, 64> mmReg[Bytes / 64];
  static constexpr int numElems = Bytes / sizeof(T), numRegs = Bytes / 64;
  Vec() = default;
  MMRegType<T, 64> &operator[](int i) { return mmReg[i]; }
  const MMRegType<T, 64> &operator[](int i) const { return mmReg[i]; }
};
  \end{minted}
  \caption{C++ implementation of the SIMD vector class}
  \label{lst:cpp_simd_vector}
\end{listing}

\begin{listing}[h!]
  \begin{minted}[
    fontsize=\fontsize{9pt}{9pt},
    linenos,
    bgcolor=white,
    numbersep=5pt
    ]{c++}
template <int Size> struct Mask {
  MaskType<Size> k;
  Mask() = default;
  Mask(const MaskType<Size> &x) : k(x) {}
  Mask &operator=(const MaskType<Size> &x) { k = x; return *this; }
  operator MaskType<Size>() const { return k; }
};
  \end{minted}
  \caption{C++ implementation of the SIMD mask class}
  \label{lst:cpp_simd_mask}
\end{listing}

\begin{listing}[h!]
  \begin{minted}[
    fontsize=\fontsize{8pt}{8pt},
    linenos,
    bgcolor=white,
    escapeinside=!!,
    numbersep=5pt
    ]{c++}
template <int Bytes = 64, typename T>
static INLINE Vec<T, Bytes> loadu(const T *p) {
  if constexpr (Bytes == 64) {
    if constexpr (std::is_integral_v<T>) { return _mm512_loadu_si512(p); }
    else if constexpr (std::is_same_v<T, float>) { return _mm512_loadu_ps(p); }
    else if constexpr (std::is_same_v<T, double>) { return _mm512_loadu_pd(p); }
  } else if constexpr (Bytes == 32) {
    if constexpr (std::is_integral_v<T>) { return _mm256_loadu_si256((__m256i_u *)p); }
    else if constexpr (std::is_same_v<T, float>) { return _mm256_loadu_ps(p); }
    else if constexpr (std::is_same_v<T, double>) { return _mm256_loadu_pd(p); }
  } else if constexpr (Bytes == 16) {
    if constexpr (std::is_integral_v<T>) { return _mm_loadu_si128((__m128i_u *)p); }
    else if constexpr (std::is_same_v<T, float>) { return _mm_loadu_ps(p); }
    else if constexpr (std::is_same_v<T, double>) { return _mm_loadu_pd(p); }
#ifdef __AVX512VL__ !\label{line:op_on_8bytes_begin}!
  } else if constexpr (Bytes == 8) {
    if constexpr (std::is_integral_v<T>) {
      if constexpr (sizeof(T) == 1) { return _mm_maskz_loadu_epi8(0xff, p); }
    }
#endif !\label{line:op_on_8bytes_end}!
  } else if constexpr (Bytes == 128 || Bytes == 256 || Bytes == 512) { !\label{line:op_on_large_begin}!
    Vec<T, Bytes> result;
    for (int i = 0; i < Vec<T, Bytes>::numRegs; i++) {
      if constexpr (std::is_integral_v<T>)
        { result[i] = _mm512_loadu_si512(p + i * Vec<T, 64>::numElems); }
      else if constexpr (std::is_same_v<T, float>)
        { result[i] = _mm512_loadu_ps(p + i * Vec<T, 64>::numElems); }
      else if constexpr (std::is_same_v<T, double>)
        { result[i] = _mm512_loadu_pd(p + i * Vec<T, 64>::numElems); }
    }
    return result;
  } !\label{line:op_on_large_end}!
}
  \end{minted}
  \caption{Generic implementation of \texttt{loadu} as a C++ template function (shortened)}
  \label{lst:cpp_simd_loadu}
\end{listing}

\chapter{SIMD Implementation of MSB Radix Sort}\label{chap:simd_radix_sort}

As mentioned in chapter~\ref{chap:cpp_implementation}, the radix recursion
function is shared by the sequential and SIMD implementations, so it is
not discussed in detail here. The part that is actually vectorized in the
SIMD implementation is the bit sorter.

The algorithm described in this chapter is called `RadixSIMD' in the following.

\section{SIMD bit sorter}

For the SIMD implementation of the bit sorter, the bit sorter developed
by \citet{moeller_radix} with a few modifications is used.
It is actually quite similar to the SIMD partitioning algorithm developed
by \citet{bramas} with the main difference being the following:
In the algorithm
developed by \citet{moeller_radix} and extended for this thesis, the elements to be
sorted are not compared to a pivot element, but instead a specific bit of the elements
is tested.\footnote{Testing a specific bit can actually be interpreted as
  comparing the elements to a `virtual' pivot element, making MSB Radix Sort
  quite similar to Quicksort, as noted by \citet[p. 128]{knuth_taocp}.}

The algorithm is first described without considering payloads and
then the handling of payloads is discussed.

In the following, \texttt{numElemsPerVec} denotes the number of elements
contained in one SIMD vector. How the value of \texttt{numElemsPerVec} and the
actual sizes of SIMD vectors are
chosen is explained in chapter~\ref{sec:choosing_numElemsPerVec}.

The C++ implementation of the SIMD bit sorter can be found in
appendix~\ref{appendix:bit_sorter_code}.

%\subsection{Main part}
\begin{algorithm}[H]
  %\footnotesize
  \caption{SIMD bit sorter}\label{algo:bit_sort_simd_key_only_main}
  \small
  \begin{algorithmic}[1]
    \Procedure{SIMDBitSort}{\texttt{array}, \texttt{left}, \texttt{right}, \texttt{bitNo}}
    \If{there are at least $\texttt{numElemsPerVec}$ elements}
    \State $\texttt{vecStore} \gets$ load \texttt{numElemsPerVec} elements from the left
    \EndIf
    \While{there are at least \texttt{numElemsPerVec} unread elements}
    \State $\texttt{vec}\gets \texttt{vecStore}$
    \State $\texttt{sortMask} \gets$ \parbox[t]{.7\linewidth}{test if bit \texttt{bitNo} is set for each element of \texttt{vec}}
    \If{there are enough elements free on the left}
    \State $\texttt{vecStore} \gets$ \parbox[t]{.7\linewidth}{load \texttt{numElemsPerVec} elements from the right}
    \Else
    \State $\texttt{vecStore} \gets$ \parbox[t]{.6\linewidth}{load \texttt{numElemsPerVec} elements from the left}
    \EndIf
    \State \parbox[t][2\baselineskip]{.9\linewidth}{store the elements in \texttt{vec} with the corresponding bit not set in \texttt{sortMask} to the left of \texttt{array}}
    \State \parbox[t][2\baselineskip]{.9\linewidth}{store the elements in \texttt{vec} with the corresponding bit set in \texttt{sortMask} to the right of \texttt{array}}
    \EndWhile
    \State $\texttt{numElemsRest}\gets $ number of remaining elements in \texttt{array}
    \If{$\texttt{numElemsRest}\neq 0$}
    \State $\texttt{vecRest}\gets$ load the remaining \texttt{numElemsRest} elements
    \EndIf
    \If{there were at least $\texttt{numElemsPerVec}$ elements in the beginning}
    \State $\texttt{sortMask} \gets$ \parbox[t]{.7\linewidth}{test if bit \texttt{bitNo} is set for each element of \texttt{vecStore}}
    \State \parbox[t][2\baselineskip]{.9\linewidth}{store the elements in \texttt{vecStore} with the corresponding bit not set in \texttt{sortMask} to the left of \texttt{array}}
    \State \parbox[t][2\baselineskip]{.9\linewidth}{store the elements in \texttt{vecStore} with the corresponding bit set in \texttt{sortMask} to the right of \texttt{array}}
    \EndIf
    \If{$\texttt{numElemsRest}\neq 0$}
    \State $\texttt{sortMaskRest} \gets$ \parbox[t]{.7\linewidth}{test bit \texttt{bitNo} for each element of \texttt{vecRest}}
    \State \parbox[t][2\baselineskip]{.9\linewidth}{store the elements in \texttt{vecRest} with the corresponding bit not set in \texttt{sortMaskRest} to the left of \texttt{array}}
    \State \parbox[t][2\baselineskip]{.9\linewidth}{store the elements in \texttt{vecRest} with the corresponding bit set in \texttt{sortMaskRest} to the right of \texttt{array}}
    \EndIf
    \State\Return index of first element with bit \texttt{bitNo} set
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Algorithm~\ref{algo:bit_sort_simd_key_only_main} shows the pseudocode for
the vectorized version of the bit sorter algorithm.
The basic idea is the same as in the sequential version shown in chapter~\ref{chap:radix_sort}:
searching for elements where the bit to sort is set to 1 from the left and
moving them to the right and searching for elements where the bit to sort is
set to 0 from the right and moving them to the left (when sorting the bit
descending this would be the other way around), essentially finding elements
that are on the `wrong' side and moving them to the `right' side.
However, instead of handling the elements one by one, multiple elements
in an entire
vector are sorted at once. This is done in the following way:

First, one SIMD vector is preloaded into \texttt{vecStore} from the left
end of the array using the \texttt{loadu} instruction, if there are at
least \texttt{numElemsPerVec} elements.
This frees\footnote
{Note that a `free' element does not mean that there is no element, but simply
  represents the fact that the element can be overwritten without losing
  any information.} \texttt{numElemsPerVec} elements on the left of
the array for sorted elements to be written to.
After that, a while loop is executed until there are less elements
left for sorting than fit into a SIMD vector.

At the beginning of each while loop iteration there are always\\
\texttt{numElemsPerVec} elements free in total, possibly not all on one side.

In the while loop, first the vector in \texttt{vecStore} is moved into
\texttt{vec}, freeing \texttt{vecStore}. Then the bit \texttt{bitNo} is tested
in each element in
\texttt{vec} using the \texttt{test\_mask} instruction,
resulting in the mask \texttt{sortMask}.
In the mask \texttt{sortMask}, a set bit indicates that the corresponding
element should be moved to the right and an unset bit indicates
that the corresponding element should be moved to the left.
If the bit \texttt{bitNo} should be sorted in descending instead of ascending
order, this should be the other way around, so \texttt{sortMask} would be
inverted in that case.

There now are \texttt{numElemsPerVec} elements in \texttt{vec} to be written
back to the array and, as noted
before, there are always \texttt{numElemsPerVec} elements free at the beginning
of a while loop iteration.
Despite this, the elements in \texttt{vec} can not yet be moved to their correct
side. This is because it is not guaranteed how these \texttt{numElemsPerVec}
free elements are distributed to the left and right sides.
Thus, there might not be enough elements free on one of the two sides.
However, on at least one side there are enough elements free, since otherwise
there would not be \texttt{numElemsPerVec} elements free in total.

So, to have enough elements free on both sides,
first the number of set bits in the mask \texttt{sortMask} is counted using the
\texttt{kpopcnt} instruction and
compared to the number of free elements
on the left and right to determine on which side there are not enough elements
free. Then, from that side, another vector of
\texttt{numElemsPerVec} elements is loaded into \texttt{vecStore} using the
\texttt{loadu} instruction
(if there are enough elements free on both sides,
the vector is still loaded, but the side from which it is loaded does not matter).

This frees another \texttt{numElemsPerVec} elements making $2\cdot \texttt{numElemsPerVec}$
elements free in total.

Since now there are enough elements free on both sides, the elements in \texttt{vec}
can be stored to the left and right, according to the mask \texttt{sortMask}.
This is done with the \texttt{mask\_compressstoreu} instructions introduced in
chapter~\ref{chap:simd_instructions}. Specifically, one \texttt{mask\_compressstoreu}
instruction is used with the mask \texttt{sortMask}, storing the elements with
the corresponding bit set in \texttt{sortMask} contiguously to the right and
another \texttt{mask\_compressstoreu} instruction with the inverted mask
\texttt{sortMask}, storing the elements with the corresponding bit not set in
\texttt{sortMask} contiguously to the left.

Now, there are \texttt{numElemsPerVec} elements free in total again and the
next while loop iteration is started.

After the while loop finished executing, there might still be
some elements left
to sort in the array because they were not enough to fill an entire SIMD vector.
Let \texttt{numElemsRest} be the number of these elements
(then $\texttt{numElemsRest} < \texttt{numElemsPerVec}$).
Additionally, if there were at least \texttt{numElemsPerVec} elements in the array
at the beginning of the while loop, there are \texttt{numElemsPerVec}
elements in \texttt{vecStore} left to sort, too.

To sort these remaining elements, first the \texttt{numElemsRest} elements
that are left in the array are loaded into \texttt{vecRest}.
This is done using the \texttt{maskz\_loadu} instruction with a mask where
only the lower \texttt{numElemsRest} bits are set. This frees
\texttt{numElemsRest} elements in the array resulting in
$\texttt{numElemsPerVec}+\texttt{numElemsRest}$ consecutive free elements
in the middle of the array.

Then, if there were at least \texttt{numElemsPerVec} elements in the array
at the beginning of the while loop, the elements in \texttt{vecStore} are
stored back into the array exactly the same way as in the while
loop, except that there are no additional elements loaded from one side,
since the $\texttt{numElemsPerVec}+\texttt{numElemsRest}$ free elements
are consecutive and thus there are enough elements free on both sides.

Finally, the remaining \texttt{numElemsRest} elements in \texttt{vecRest} are
stored back into the array, again exactly the same way as in the while
loop, except that there are no additional elements loaded from one side.


\section{Example for the SIMD bit sorter}

To aid with the understanding of the SIMD bit sorter algorithm,
this section provides an example.

In the example, an array of length 15 is sorted according to the
\texttt{bitNo}-th bit of the key. Instead of whole elements,
only the \texttt{bitNo}-th bit of the key
of each element is used to represent the whole element.
Free elements are represented by `{\tiny \texttt{/}}'. Note that, as above,
`free' does not mean that there is no element, but simply
represents the fact that the element can be overwritten without losing
any information.
For the example, \texttt{numElemsPerVec} is 4.

\begin{figure}[h!]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {1)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {0};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {0};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {1};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {0};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {1};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {0};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {1};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {0};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {0};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {1};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {0};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {1};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {1};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {0};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {0};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {2)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {\tiny \texttt{/}};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {\tiny \texttt{/}};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {\tiny \texttt{/}};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {\tiny \texttt{/}};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {1};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {0};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {1};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {0};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {0};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {1};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {0};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {1};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {1};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {0};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {0};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {0};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {0};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {1};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {0};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {3)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {\tiny \texttt{/}};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {\tiny \texttt{/}};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {\tiny \texttt{/}};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {\tiny \texttt{/}};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {1};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {0};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {1};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {0};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {0};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {1};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {0};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {1};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {1};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {0};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {0};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {\tiny \texttt{/}};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {\tiny \texttt{/}};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {\tiny \texttt{/}};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {\tiny \texttt{/}};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);

      \node[anchor=east] at (8.75,-1) {\texttt{vec}};
      \draw (9,-1.5) -- (9,-0.5);
      \node[anchor=center] at (9.5,-1) {0};
      \draw (9,-1.5) -- (10,-1.5) -- (10,-0.5) -- (9,-0.5);
      \node[anchor=center] at (10.5,-1) {0};
      \draw (10,-1.5) -- (11,-1.5) -- (11,-0.5) -- (10,-0.5);
      \node[anchor=center] at (11.5,-1) {1};
      \draw (11,-1.5) -- (12,-1.5) -- (12,-0.5) -- (11,-0.5);
      \node[anchor=center] at (12.5,-1) {0};
      \draw (12,-1.5) -- (13,-1.5) -- (13,-0.5) -- (12,-0.5);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {4)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {\tiny \texttt{/}};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {\tiny \texttt{/}};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {\tiny \texttt{/}};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {\tiny \texttt{/}};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {1};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {0};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {1};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {0};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {0};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {1};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {0};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {\tiny \texttt{/}};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {\tiny \texttt{/}};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {\tiny \texttt{/}};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {\tiny \texttt{/}};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {1};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {1};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {0};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {0};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);

      \node[anchor=east] at (8.75,-1) {\texttt{vec}};
      \draw (9,-1.5) -- (9,-0.5);
      \node[anchor=center] at (9.5,-1) {0};
      \draw (9,-1.5) -- (10,-1.5) -- (10,-0.5) -- (9,-0.5);
      \node[anchor=center] at (10.5,-1) {0};
      \draw (10,-1.5) -- (11,-1.5) -- (11,-0.5) -- (10,-0.5);
      \node[anchor=center] at (11.5,-1) {1};
      \draw (11,-1.5) -- (12,-1.5) -- (12,-0.5) -- (11,-0.5);
      \node[anchor=center] at (12.5,-1) {0};
      \draw (12,-1.5) -- (13,-1.5) -- (13,-0.5) -- (12,-0.5);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {5)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {0};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {0};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {0};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {\tiny \texttt{/}};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {1};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {0};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {1};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {0};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {0};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {1};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {0};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {\tiny \texttt{/}};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {\tiny \texttt{/}};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {\tiny \texttt{/}};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {1};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {1};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {1};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {0};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {0};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);

      \node[anchor=east] at (8.75,-1) {\texttt{vec}};
      \draw (9,-1.5) -- (9,-0.5);
      \node[anchor=center] at (9.5,-1) {\tiny \texttt{/}};
      \draw (9,-1.5) -- (10,-1.5) -- (10,-0.5) -- (9,-0.5);
      \node[anchor=center] at (10.5,-1) {\tiny \texttt{/}};
      \draw (10,-1.5) -- (11,-1.5) -- (11,-0.5) -- (10,-0.5);
      \node[anchor=center] at (11.5,-1) {\tiny \texttt{/}};
      \draw (11,-1.5) -- (12,-1.5) -- (12,-0.5) -- (11,-0.5);
      \node[anchor=center] at (12.5,-1) {\tiny \texttt{/}};
      \draw (12,-1.5) -- (13,-1.5) -- (13,-0.5) -- (12,-0.5);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {6)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {0};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {0};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {0};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {\tiny \texttt{/}};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {1};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {0};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {1};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {0};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {0};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {1};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {0};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {\tiny \texttt{/}};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {\tiny \texttt{/}};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {\tiny \texttt{/}};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {1};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {\tiny \texttt{/}};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {\tiny \texttt{/}};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {\tiny \texttt{/}};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {\tiny \texttt{/}};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);

      \node[anchor=east] at (8.75,-1) {\texttt{vec}};
      \draw (9,-1.5) -- (9,-0.5);
      \node[anchor=center] at (9.5,-1) {1};
      \draw (9,-1.5) -- (10,-1.5) -- (10,-0.5) -- (9,-0.5);
      \node[anchor=center] at (10.5,-1) {1};
      \draw (10,-1.5) -- (11,-1.5) -- (11,-0.5) -- (10,-0.5);
      \node[anchor=center] at (11.5,-1) {0};
      \draw (11,-1.5) -- (12,-1.5) -- (12,-0.5) -- (11,-0.5);
      \node[anchor=center] at (12.5,-1) {0};
      \draw (12,-1.5) -- (13,-1.5) -- (13,-0.5) -- (12,-0.5);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {7)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {0};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {0};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {0};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {\tiny \texttt{/}};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {\tiny \texttt{/}};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {\tiny \texttt{/}};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {\tiny \texttt{/}};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {\tiny \texttt{/}};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {0};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {1};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {0};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {\tiny \texttt{/}};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {\tiny \texttt{/}};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {\tiny \texttt{/}};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {1};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {1};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {0};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {1};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {0};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);

      \node[anchor=east] at (8.75,-1) {\texttt{vec}};
      \draw (9,-1.5) -- (9,-0.5);
      \node[anchor=center] at (9.5,-1) {1};
      \draw (9,-1.5) -- (10,-1.5) -- (10,-0.5) -- (9,-0.5);
      \node[anchor=center] at (10.5,-1) {1};
      \draw (10,-1.5) -- (11,-1.5) -- (11,-0.5) -- (10,-0.5);
      \node[anchor=center] at (11.5,-1) {0};
      \draw (11,-1.5) -- (12,-1.5) -- (12,-0.5) -- (11,-0.5);
      \node[anchor=center] at (12.5,-1) {0};
      \draw (12,-1.5) -- (13,-1.5) -- (13,-0.5) -- (12,-0.5);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {8)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {0};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {0};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {0};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {0};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {0};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {\tiny \texttt{/}};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {\tiny \texttt{/}};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {\tiny \texttt{/}};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {0};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {1};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {0};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {\tiny \texttt{/}};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {1};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {1};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {1};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {1};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {0};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {1};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {0};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);

      \node[anchor=east] at (8.75,-1) {\texttt{vec}};
      \draw (9,-1.5) -- (9,-0.5);
      \node[anchor=center] at (9.5,-1) {\tiny \texttt{/}};
      \draw (9,-1.5) -- (10,-1.5) -- (10,-0.5) -- (9,-0.5);
      \node[anchor=center] at (10.5,-1) {\tiny \texttt{/}};
      \draw (10,-1.5) -- (11,-1.5) -- (11,-0.5) -- (10,-0.5);
      \node[anchor=center] at (11.5,-1) {\tiny \texttt{/}};
      \draw (11,-1.5) -- (12,-1.5) -- (12,-0.5) -- (11,-0.5);
      \node[anchor=center] at (12.5,-1) {\tiny \texttt{/}};
      \draw (12,-1.5) -- (13,-1.5) -- (13,-0.5) -- (12,-0.5);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {9)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {0};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {0};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {0};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {0};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {0};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {\tiny \texttt{/}};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {\tiny \texttt{/}};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {\tiny \texttt{/}};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {\tiny \texttt{/}};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {\tiny \texttt{/}};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {\tiny \texttt{/}};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {\tiny \texttt{/}};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {1};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {1};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {1};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {1};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {0};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {1};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {0};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);

      \node[anchor=east] at (8.75,-1) {\texttt{vecRest}};
      \draw (9,-1.5) -- (9,-0.5);
      \node[anchor=center] at (9.5,-1) {0};
      \draw (9,-1.5) -- (10,-1.5) -- (10,-0.5) -- (9,-0.5);
      \node[anchor=center] at (10.5,-1) {1};
      \draw (10,-1.5) -- (11,-1.5) -- (11,-0.5) -- (10,-0.5);
      \node[anchor=center] at (11.5,-1) {0};
      \draw (11,-1.5) -- (12,-1.5) -- (12,-0.5) -- (11,-0.5);
      \node[anchor=center] at (12.5,-1) {0};
      \fill[fill=lightgray,opacity=0.5] (12,-1.5) rectangle (13,-0.5);
      \draw (12,-1.5) -- (13,-1.5) -- (13,-0.5) -- (12,-0.5);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {10)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {0};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {0};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {0};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {0};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {0};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {0};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {0};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {\tiny \texttt{/}};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {\tiny \texttt{/}};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {\tiny \texttt{/}};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {1};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {1};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {1};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {1};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {1};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {\tiny \texttt{/}};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {\tiny \texttt{/}};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {\tiny \texttt{/}};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {\tiny \texttt{/}};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);

      \node[anchor=east] at (8.75,-1) {\texttt{vecRest}};
      \draw (9,-1.5) -- (9,-0.5);
      \node[anchor=center] at (9.5,-1) {0};
      \draw (9,-1.5) -- (10,-1.5) -- (10,-0.5) -- (9,-0.5);
      \node[anchor=center] at (10.5,-1) {1};
      \draw (10,-1.5) -- (11,-1.5) -- (11,-0.5) -- (10,-0.5);
      \node[anchor=center] at (11.5,-1) {0};
      \draw (11,-1.5) -- (12,-1.5) -- (12,-0.5) -- (11,-0.5);
      \node[anchor=center] at (12.5,-1) {0};
      \fill[fill=lightgray,opacity=0.5] (12,-1.5) rectangle (13,-0.5);
      \draw (12,-1.5) -- (13,-1.5) -- (13,-0.5) -- (12,-0.5);
    \end{tikzpicture}
  \end{subfigure}\\
  \vspace*{0.5\baselineskip}
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.52]
      \node[anchor=west] at (-6,0.5) {11)};
      \node[anchor=east] at (-0.25,0.5) {array};
      \draw (0,0) -- (0,1);
      \node[anchor=center] at (0.5,0.5) {0};
      \draw (0,0) -- (1,0) -- (1,1) -- (0,1);
      \node[anchor=center] at (1.5,0.5) {0};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=center] at (2.5,0.5) {0};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=center] at (3.5,0.5) {0};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=center] at (4.5,0.5) {0};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=center] at (5.5,0.5) {0};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=center] at (6.5,0.5) {0};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=center] at (7.5,0.5) {0};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=center] at (8.5,0.5) {0};
      \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
      \node[anchor=center] at (9.5,0.5) {1};
      \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
      \node[anchor=center] at (10.5,0.5) {1};
      \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
      \node[anchor=center] at (11.5,0.5) {1};
      \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
      \node[anchor=center] at (12.5,0.5) {1};
      \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
      \node[anchor=center] at (13.5,0.5) {1};
      \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
      \node[anchor=center] at (14.5,0.5) {1};
      \draw (14,0) -- (15,0) -- (15,1) -- (14,1);

      \node[anchor=east] at (-0.25,-1) {\texttt{vecStore}};
      \draw (0,-1.5) -- (0,-0.5);
      \node[anchor=center] at (0.5,-1) {\tiny \texttt{/}};
      \draw (0,-1.5) -- (1,-1.5) -- (1,-0.5) -- (0,-0.5);
      \node[anchor=center] at (1.5,-1) {\tiny \texttt{/}};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-0.5) -- (1,-0.5);
      \node[anchor=center] at (2.5,-1) {\tiny \texttt{/}};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-0.5) -- (2,-0.5);
      \node[anchor=center] at (3.5,-1) {\tiny \texttt{/}};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-0.5) -- (3,-0.5);

      \node[anchor=east] at (8.75,-1) {\texttt{vecRest}};
      \draw (9,-1.5) -- (9,-0.5);
      \node[anchor=center] at (9.5,-1) {\tiny \texttt{/}};
      \draw (9,-1.5) -- (10,-1.5) -- (10,-0.5) -- (9,-0.5);
      \node[anchor=center] at (10.5,-1) {\tiny \texttt{/}};
      \draw (10,-1.5) -- (11,-1.5) -- (11,-0.5) -- (10,-0.5);
      \node[anchor=center] at (11.5,-1) {\tiny \texttt{/}};
      \draw (11,-1.5) -- (12,-1.5) -- (12,-0.5) -- (11,-0.5);
      \node[anchor=center] at (12.5,-1) {\tiny \texttt{/}};
      \draw (12,-1.5) -- (13,-1.5) -- (13,-0.5) -- (12,-0.5);
    \end{tikzpicture}
  \end{subfigure}
  \caption{Example of the SIMD bit sorter}
  \label{fig:bit_sort_example}
\end{figure}

Figure~\ref{fig:bit_sort_example} shows the example of the SIMD bit sorter.
The steps are as follows:

\begin{enumerate}[label=\arabic*)]
  \item The initial state. The array is not sorted according to bit
        \texttt{bitNo} yet.
  \item A vector is preloaded from the left into \texttt{vecStore}.
  \item Beginning of first while loop iteration: the vector in \texttt{vecStore}
        is moved to \texttt{vec}.
  \item 3 elements in \texttt{vec} should go to the right, but there are no elements free on the right,
        so a vector is loaded into \texttt{vecStore} from the right.
  \item The elements in \texttt{vec} are stored to the left and right, according to bit \texttt{bitNo}
        using the \texttt{mask\_compressstoreu} instruction.
  \item Beginning of second while loop iteration: the vector in \texttt{vecStore}
        is moved to \texttt{vec}.
  \item 2 elements in \texttt{vec} should go to the left, but there is only one element
        free on the left, so a vector is loaded into \texttt{vecStore} from the left.
  \item The elements in \texttt{vec} are stored to the left and right, according to bit \texttt{bitNo}
        using the \texttt{mask\_compressstoreu} instruction.
  \item There are now $\texttt{numElemsRest}=3<4=\texttt{numElemsPerVec}$ remaining elements in the
        array left to read, so the while loop is exited and the remaining elements are loaded into \texttt{vecRest}
        using the \texttt{maskz\_loadu} instruction. The greyed out 0-elements
        in \texttt{vecRest} represent the elements that were set to 0 by the \texttt{maskz\_loadu}
        instruction.
  \item The elements in \texttt{vecStore} are stored to the left and right, according to bit \texttt{bitNo}
        using the \texttt{mask\_compressstoreu} instruction.
  \item The elements in \texttt{vecRest} are stored to the left and right, according to bit \texttt{bitNo}
        using the \texttt{mask\_compressstoreu} instruction.\\
        The array is now sorted according to bit \texttt{bitNo}.
\end{enumerate}

\section{Difference to previous implementation}

The implementation of the bit sorter
presented in this thesis is different
than the implementation by \citet{moeller_radix}.
If the number of elements in the subarray is an integer multiple of
\texttt{numElemsPerVec}, the two implementations are similar.
However if it is not, the two implementations handle the remaining elements
differently.

The implementation by \citet{moeller_radix} leaves the remaining elements
on the right of the subarray and sorts the part that is an integer multiple of
\texttt{numElemsPerVec} using SIMD instructions. It then sorts the remaining
elements into the `middle' of the already sorted part by using
a modified sequential bit sorter. So the SIMD bit sorter by \citet{moeller_radix}
essentially `sweeps' the array from the left and almost the right into
the middle and then accesses the right end of the array again, which might
cause a cache miss and thus suboptimal performance.
The implementation developed in this thesis instead leaves the remaining
elements in the `middle' and then uses masked instructions to sort them.
It essentially `sweeps' the array from the left and right into the middle
and then only accesses the middle to sort the remaining elements,
which prevents a potential cache miss and thus might increase performance.
Additionally, using masked SIMD instructions instead of a sequential bit sorter
probably also increases performance.

\section{Handling of Payloads}

So far, the sorting has only been discussed without considering payloads.
Therefore, so far the SIMD implementation of MSB Radix Sort can only sort
datasets which do not contain payloads.
But one may also want to sort a dataset where a single element consists of
a key and one or more payloads rather than just one key.

\subsection{Previous Method}
\label{sec:payload_handling_previous_method}

%\begin{figure}
%  \centering
%  
%  \caption{Usage of an 512-bit AVX-512 vector register when sorting a 64-bit key
%    (represented with K) with a 64-bit payload (represented with P)
%    using the algorithm by \citet{moeller_radix}. The
%    striped areas represent unused bits.}
%  \label{fig:key_payload_moeller_small_payload}
%\end{figure}

The implementation by \citet{moeller_radix} handles payloads by combining key
and payload into
a single element, where the key occupies the low bytes and the payload
occupies the high bytes.
This results in a single array where keys and payloads are interleaved,
also called array of structs (AoS) \citep{intel_aos}.
The sorting is then done by loading the keys and payloads into a
vector register and only considering
the bits of the element where the key is located, i.e. the low bits.

This however means that, in contrast to key-only sorting, only half
of one SIMD vector instead of a full vector is filled with keys,
as can be seen in figures~\ref{fig:register_usage_moeller_key_only} and
\ref{fig:register_usage_moeller_with_payload}.
This reduces the number of elements per vector, which reduces the speedup
gained by the SIMD implementation compared to key-only sorting significantly
\citep{moeller_radix}.

Additionally, the implementation by \citet{moeller_radix} only supports at most
one payload, which also has to have the same size as the key.
His implementation can also be used to sort a dataset where the payload is
smaller than the key by padding the payload to the size of the key.
However this wastes parts of the vector registers used for sorting
as shown in figure~\ref{fig:register_usage_moeller_with_half_payload}.

\begin{figure}[h!]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.75]
      \node[anchor=mid] at (-0.7,1.5) {\small bit};
      \draw (0,1) -- (0,1.2);
      \node[anchor=mid] at (0,1.5) {0};
      \draw (0,0) -- (0,1) -- (2,1) -- (2,0) -- cycle;
      \node[anchor=mid] at (1,0.5) {$K_0$};
      \draw (2,1) -- (2,1.2);
      \node[anchor=mid] at (2,1.5) {64};
      \draw (2,1) -- (4,1) -- (4,0) -- (2,0);
      \node[anchor=mid] at (3,0.5) {$K_1$};
      \draw (4,1) -- (4,1.2);
      \node[anchor=mid] at (4,1.5) {128};
      \draw (4,1) -- (6,1) -- (6,0) -- (4,0);
      \node[anchor=mid] at (5,0.5) {$K_2$};
      \draw (6,1) -- (6,1.2);
      \node[anchor=mid] at (6,1.5) {192};
      \draw (6,1) -- (8,1) -- (8,0) -- (6,0);
      \node[anchor=mid] at (7,0.5) {$K_3$};
      \draw (8,1) -- (8,1.2);
      \node[anchor=mid] at (8,1.5) {256};
      \draw (8,1) -- (10,1) -- (10,0) -- (8,0);
      \node[anchor=mid] at (9,0.5) {$K_4$};
      \draw (10,1) -- (10,1.2);
      \node[anchor=mid] at (10,1.5) {320};
      \draw (10,1) -- (12,1) -- (12,0) -- (10,0);
      \node[anchor=mid] at (11,0.5) {$K_5$};
      \draw (12,1) -- (12,1.2);
      \node[anchor=mid] at (12,1.5) {384};
      \draw (12,1) -- (14,1) -- (14,0) -- (12,0);
      \node[anchor=mid] at (13,0.5) {$K_6$};
      \draw (14,1) -- (14,1.2);
      \node[anchor=mid] at (14,1.5) {448};
      \draw (14,1) -- (16,1) -- (16,0) -- (14,0);
      \node[anchor=mid] at (15,0.5) {$K_7$};
      \draw (16,1) -- (16,1.2);
      \node[anchor=mid] at (16,1.5) {512};
    \end{tikzpicture}
    \caption{key only, 8 elements fit into one vector}
    \label{fig:register_usage_moeller_key_only}
  \end{subfigure}\\

  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.75]
      \node[anchor=mid] at (-0.7,1.5) {\small bit};
      \draw (0,1) -- (0,1.2);
      \node[anchor=mid] at (0,1.5) {0};
      \draw (0,0) -- (0,1) -- (2,1) -- (2,0) -- cycle;
      \node[anchor=mid] at (1,0.5) {$K_0$};
      \draw (2,1) -- (2,1.2);
      \node[anchor=mid] at (2,1.5) {64};
      \draw (2,1) -- (4,1) -- (4,0) -- (2,0);
      \node[anchor=mid] at (3,0.5) {$P_0$};
      \draw (4,1) -- (4,1.2);
      \node[anchor=mid] at (4,1.5) {128};
      \draw (4,1) -- (6,1) -- (6,0) -- (4,0);
      \node[anchor=mid] at (5,0.5) {$K_1$};
      \draw (6,1) -- (6,1.2);
      \node[anchor=mid] at (6,1.5) {192};
      \draw (6,1) -- (8,1) -- (8,0) -- (6,0);
      \node[anchor=mid] at (7,0.5) {$P_1$};
      \draw (8,1) -- (8,1.2);
      \node[anchor=mid] at (8,1.5) {256};
      \draw (8,1) -- (10,1) -- (10,0) -- (8,0);
      \node[anchor=mid] at (9,0.5) {$K_2$};
      \draw (10,1) -- (10,1.2);
      \node[anchor=mid] at (10,1.5) {320};
      \draw (10,1) -- (12,1) -- (12,0) -- (10,0);
      \node[anchor=mid] at (11,0.5) {$P_2$};
      \draw (12,1) -- (12,1.2);
      \node[anchor=mid] at (12,1.5) {384};
      \draw (12,1) -- (14,1) -- (14,0) -- (12,0);
      \node[anchor=mid] at (13,0.5) {$K_3$};
      \draw (14,1) -- (14,1.2);
      \node[anchor=mid] at (14,1.5) {448};
      \draw (14,1) -- (16,1) -- (16,0) -- (14,0);
      \node[anchor=mid] at (15,0.5) {$P_3$};
      \draw (16,1) -- (16,1.2);
      \node[anchor=mid] at (16,1.5) {512};
    \end{tikzpicture}
    \caption{with payload the same size as the key, only 4 elements fit into one vector}
    \label{fig:register_usage_moeller_with_payload}
  \end{subfigure}\\
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[scale=0.75]
      \node[anchor=mid] at (-0.7,1.5) {\small bit};
      \draw (0,1) -- (0,1.2);
      \node[anchor=mid] at (0,1.5) {0};
      \draw (0,0) -- (0,1) -- (2,1) -- (2,0) -- cycle;
      \node[anchor=mid] at (1,0.5) {$K_0$};
      \draw (2,1) -- (2,1.2);
      \node[anchor=mid] at (2,1.5) {64};
      \draw (2,1) -- (4,1) -- (4,0) -- (2,0);
      \node[anchor=mid] at (2.5,0.5) {$P_0$};
      \draw (3,0) -- (3,1.2);
      \node[anchor=mid] at (3,1.5) {96};
      \fill[pattern=north east lines] (3,0) rectangle (4,1);
      \draw (4,1) -- (4,1.2);
      \node[anchor=mid] at (4,1.5) {128};
      \draw (4,1) -- (6,1) -- (6,0) -- (4,0);
      \node[anchor=mid] at (5,0.5) {$K_1$};
      \draw (6,1) -- (6,1.2);
      \node[anchor=mid] at (6,1.5) {192};
      \draw (6,1) -- (8,1) -- (8,0) -- (6,0);
      \node[anchor=mid] at (6.5,0.5) {$P_1$};
      \draw (7,0) -- (7,1.2);
      \node[anchor=mid] at (7,1.5) {224};
      \fill[pattern=north east lines] (7,0) rectangle (8,1);
      \draw (8,1) -- (8,1.2);
      \node[anchor=mid] at (8,1.5) {256};
      \draw (8,1) -- (10,1) -- (10,0) -- (8,0);
      \node[anchor=mid] at (9,0.5) {$K_2$};
      \draw (10,1) -- (10,1.2);
      \node[anchor=mid] at (10,1.5) {320};
      \draw (10,1) -- (12,1) -- (12,0) -- (10,0);
      \node[anchor=mid] at (10.5,0.5) {$P_2$};
      \draw (11,0) -- (11,1.2);
      \node[anchor=mid] at (11,1.5) {352};
      \fill[pattern=north east lines] (11,0) rectangle (12,1);
      \draw (12,1) -- (12,1.2);
      \node[anchor=mid] at (12,1.5) {384};
      \draw (12,1) -- (14,1) -- (14,0) -- (12,0);
      \node[anchor=mid] at (13,0.5) {$K_3$};
      \draw (14,1) -- (14,1.2);
      \node[anchor=mid] at (14,1.5) {448};
      \draw (14,1) -- (16,1) -- (16,0) -- (14,0);
      \node[anchor=mid] at (14.5,0.5) {$P_3$};
      \draw (15,0) -- (15,1.2);
      \node[anchor=mid] at (15,1.5) {480};
      \fill[pattern=north east lines] (15,0) rectangle (16,1);
      \draw (16,1) -- (16,1.2);
      \node[anchor=mid] at (16,1.5) {512};
    \end{tikzpicture}
    \caption{with payload half the size of the key,
      only 4 elements fit into one vector and there is unused space (striped area)}
    \label{fig:register_usage_moeller_with_half_payload}
  \end{subfigure}
  \caption{Usage of a 512-bit AVX-512 vector register when sorting a 64-bit key
    (represented with $K_i$) with different payloads
    (represented with $P_i$) using the algorithm by \citet{moeller_radix}.}
  \label{fig:register_usage_moeller}
\end{figure}

To remove these limitations and improve performance when sorting
datasets with payloads, a new method is proposed.

\subsection{Handling different numbers of payloads}

To be able to always sort the same number of elements in one vector regardless
of the number of payloads, the proposed method uses separate arrays
instead of arranging the data in one interleaved array.
Specifically, one array is used for the keys
and an additional array is used for each payload.
This data structure is called SoA (Structure of Arrays) \citep{intel_aos}.

The sorting is then done on the key array exactly as in the key-only case except
that every load and store operation that is performed on the key array is mirrored
for every payload array. For the \texttt{mask\_compressstoreu} instruction, the same sort masks
that were obtained from the keys are used. So the key and every payload are loaded into
separate vector registers, resulting in a full SIMD vector being filled with keys
as shown in figure~\ref{fig:register_usage_two_payloads}.
This way, the same number of elements can be sorted at the same time as in the
key-only case, independent of the number of payloads.


\begin{figure}[h]
  \centering
  \begin{tikzpicture}[yscale=0.75, xscale=0.625]
    \node[anchor=east] at (-0.5,0.5) {key vector};
    \node[anchor=mid] at (-0.8,1.5) {\small bit};
    \draw (0,1) -- (0,1.2);
    \node[anchor=mid] at (0,1.5) {0};
    \draw (0,0) -- (0,1) -- (2,1) -- (2,0) -- cycle;
    \node[anchor=mid] at (1,0.5) {$K_0$};
    \draw (2,1) -- (2,1.2);
    \node[anchor=mid] at (2,1.5) {64};
    \draw (2,1) -- (4,1) -- (4,0) -- (2,0);
    \node[anchor=mid] at (3,0.5) {$K_1$};
    \draw (4,1) -- (4,1.2);
    \node[anchor=mid] at (4,1.5) {128};
    \draw (4,1) -- (6,1) -- (6,0) -- (4,0);
    \node[anchor=mid] at (5,0.5) {$K_2$};
    \draw (6,1) -- (6,1.2);
    \node[anchor=mid] at (6,1.5) {192};
    \draw (6,1) -- (8,1) -- (8,0) -- (6,0);
    \node[anchor=mid] at (7,0.5) {$K_3$};
    \draw (8,1) -- (8,1.2);
    \node[anchor=mid] at (8,1.5) {256};
    \draw (8,1) -- (10,1) -- (10,0) -- (8,0);
    \node[anchor=mid] at (9,0.5) {$K_4$};
    \draw (10,1) -- (10,1.2);
    \node[anchor=mid] at (10,1.5) {320};
    \draw (10,1) -- (12,1) -- (12,0) -- (10,0);
    \node[anchor=mid] at (11,0.5) {$K_5$};
    \draw (12,1) -- (12,1.2);
    \node[anchor=mid] at (12,1.5) {384};
    \draw (12,1) -- (14,1) -- (14,0) -- (12,0);
    \node[anchor=mid] at (13,0.5) {$K_6$};
    \draw (14,1) -- (14,1.2);
    \node[anchor=mid] at (14,1.5) {448};
    \draw (14,1) -- (16,1) -- (16,0) -- (14,0);
    \node[anchor=mid] at (15,0.5) {$K_7$};
    \draw (16,1) -- (16,1.2);
    \node[anchor=mid] at (16,1.5) {512};

    \node[anchor=east] at (-0.5,-2) {\shortstack[l]{first\\payload vector}};
    \node[anchor=mid] at (-0.8,-1) {\small bit};
    \draw (0,-1.5) -- (0,-1.3);
    \node[anchor=mid] at (0,-1) {0};
    \draw (0,-2.5) -- (0,-1.5) -- (2,-1.5) -- (2,-2.5) -- cycle;
    \node[anchor=mid] at (1,-2) {$P^{(0)}_0$};
    \draw (2,-1.5) -- (2,-1.3);
    \node[anchor=mid] at (2,-1) {64};
    \draw (2,-2.5) -- (4,-2.5) -- (4,-1.5) -- (2,-1.5);
    \node[anchor=mid] at (3,-2) {$P^{(0)}_1$};
    \draw (4,-1.5) -- (4,-1.3);
    \node[anchor=mid] at (4,-1) {128};
    \draw (4,-2.5) -- (6,-2.5) -- (6,-1.5) -- (4,-1.5);
    \node[anchor=mid] at (5,-2) {$P^{(0)}_2$};
    \draw (6,-1.5) -- (6,-1.3);
    \node[anchor=mid] at (6,-1) {192};
    \draw (6,-2.5) -- (8,-2.5) -- (8,-1.5) -- (6,-1.5);
    \node[anchor=mid] at (7,-2) {$P^{(0)}_3$};
    \draw (8,-1.5) -- (8,-1.3);
    \node[anchor=mid] at (8,-1) {256};
    \draw (8,-2.5) -- (10,-2.5) -- (10,-1.5) -- (8,-1.5);
    \node[anchor=mid] at (9,-2) {$P^{(0)}_4$};
    \draw (10,-1.5) -- (10,-1.3);
    \node[anchor=mid] at (10,-1) {320};
    \draw (10,-2.5) -- (12,-2.5) -- (12,-1.5) -- (10,-1.5);
    \node[anchor=mid] at (11,-2) {$P^{(0)}_5$};
    \draw (12,-1.5) -- (12,-1.3);
    \node[anchor=mid] at (12,-1) {384};
    \draw (12,-2.5) -- (14,-2.5) -- (14,-1.5) -- (12,-1.5);
    \node[anchor=mid] at (13,-2) {$P^{(0)}_6$};
    \draw (14,-1.5) -- (14,-1.3);
    \node[anchor=mid] at (14,-1) {448};
    \draw (14,-2.5) -- (16,-2.5) -- (16,-1.5) -- (14,-1.5);
    \node[anchor=mid] at (15,-2) {$P^{(0)}_7$};
    \draw (16,-1.5) -- (16,-1.3);
    \node[anchor=mid] at (16,-1) {512};

    \node[anchor=east] at (-0.5,-4.5) {\shortstack[l]{second\\payload vector}};
    \node[anchor=mid] at (-0.8,-3.5) {\small bit};
    \draw (0,-4) -- (0,-3.8);
    \node[anchor=mid] at (0,-3.5) {0};
    \draw (0,-5) -- (0,-4) -- (2,-4) -- (2,-5) -- cycle;
    \node[anchor=mid] at (1,-4.5) {$P^{(1)}_0$};
    \draw (2,-4) -- (2,-3.8);
    \node[anchor=mid] at (2,-3.5) {64};
    \draw (2,-5) -- (4,-5) -- (4,-4) -- (2,-4);
    \node[anchor=mid] at (3,-4.5) {$P^{(1)}_1$};
    \draw (4,-4) -- (4,-3.8);
    \node[anchor=mid] at (4,-3.5) {128};
    \draw (4,-5) -- (6,-5) -- (6,-4) -- (4,-4);
    \node[anchor=mid] at (5,-4.5) {$P^{(1)}_2$};
    \draw (6,-4) -- (6,-3.8);
    \node[anchor=mid] at (6,-3.5) {192};
    \draw (6,-5) -- (8,-5) -- (8,-4) -- (6,-4);
    \node[anchor=mid] at (7,-4.5) {$P^{(1)}_3$};
    \draw (8,-4) -- (8,-3.8);
    \node[anchor=mid] at (8,-3.5) {256};
    \draw (8,-5) -- (10,-5) -- (10,-4) -- (8,-4);
    \node[anchor=mid] at (9,-4.5) {$P^{(1)}_4$};
    \draw (10,-4) -- (10,-3.8);
    \node[anchor=mid] at (10,-3.5) {320};
    \draw (10,-5) -- (12,-5) -- (12,-4) -- (10,-4);
    \node[anchor=mid] at (11,-4.5) {$P^{(1)}_5$};
    \draw (12,-4) -- (12,-3.8);
    \node[anchor=mid] at (12,-3.5) {384};
    \draw (12,-5) -- (14,-5) -- (14,-4) -- (12,-4);
    \node[anchor=mid] at (13,-4.5) {$P^{(1)}_6$};
    \draw (14,-4) -- (14,-3.8);
    \node[anchor=mid] at (14,-3.5) {448};
    \draw (14,-5) -- (16,-5) -- (16,-4) -- (14,-4);
    \node[anchor=mid] at (15,-4.5) {$P^{(1)}_7$};
    \draw (16,-4) -- (16,-3.8);
    \node[anchor=mid] at (16,-3.5) {512};
  \end{tikzpicture}
  \caption{Usage of 512-bit AVX-512 vector registers when sorting a 64-bit key
    (represented with $K_i$) with two 64-bit payloads
    (represented with $P^{(0)}_i$ and $P^{(1)}_i$) using the proposed method.}
  \label{fig:register_usage_two_payloads}
\end{figure}

Thus, the only slowdown when sorting a dataset with more or larger payloads is due
to the cost of the additional load and store operations for the payloads, instead
of also increasing the element-size and thereby reducing the number of elements
that fit into a single SIMD vector.

This is the same way the sorting algorithm by \citet{bramas} handles payloads.
However, his algorithm only supports one \texttt{int32} payload and only
if the key also has the type \texttt{int32} as well.

\subsection{Handling different sized key and payloads and choosing \texttt{numElemsPerVec}}
\label{sec:choosing_numElemsPerVec}

So far, only the sorting of datasets where all payloads have the same size as
the key was discussed. However, one might also want to sort datasets where
some or all payloads have a size that differs from the size of the key.
One simple way to achieve this would be to pad all smaller types to the size of the
largest type. However, this wastes space both in memory and in the vector
registers as seen in section~\ref{sec:payload_handling_previous_method}, where
a smaller payload was padded to the size of the key to be sorted by the algorithm
by \citet{moeller_radix}.

To avoid wasted space, instead of padding the data type, one could simply use
smaller vector registers as shown in figure~\ref{fig:register_usage_different_payload}.
When all payloads are smaller than or as big as the key, this is exactly
what is done in the algorithm developed for this thesis.
Here, the vectors with sizes below 512 bits that were mentioned in
section~\ref{sec:template_wrapper}
are necessary. The smallest vector with size 64 bits (or 8 bytes) that
had to be emulated is necessary when sorting a dataset with 64-bit keys and
8-bit payloads.

\begin{figure}[h]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[yscale=0.75, xscale=0.625]
      \node[anchor=east] at (-0.5,0.5) {key vector};
      \node[anchor=mid] at (-0.8,1.5) {\small bit};
      \draw (0,1) -- (0,1.2);
      \node[anchor=mid] at (0,1.5) {0};
      \draw (0,0) -- (0,1) -- (2,1) -- (2,0) -- cycle;
      \node[anchor=mid] at (1,0.5) {$K_0$};
      \draw (2,1) -- (2,1.2);
      \node[anchor=mid] at (2,1.5) {64};
      \draw (2,1) -- (4,1) -- (4,0) -- (2,0);
      \node[anchor=mid] at (3,0.5) {$K_1$};
      \draw (4,1) -- (4,1.2);
      \node[anchor=mid] at (4,1.5) {128};
      \draw (4,1) -- (6,1) -- (6,0) -- (4,0);
      \node[anchor=mid] at (5,0.5) {$K_2$};
      \draw (6,1) -- (6,1.2);
      \node[anchor=mid] at (6,1.5) {192};
      \draw (6,1) -- (8,1) -- (8,0) -- (6,0);
      \node[anchor=mid] at (7,0.5) {$K_3$};
      \draw (8,1) -- (8,1.2);
      \node[anchor=mid] at (8,1.5) {256};
      \draw (8,1) -- (10,1) -- (10,0) -- (8,0);
      \node[anchor=mid] at (9,0.5) {$K_4$};
      \draw (10,1) -- (10,1.2);
      \node[anchor=mid] at (10,1.5) {320};
      \draw (10,1) -- (12,1) -- (12,0) -- (10,0);
      \node[anchor=mid] at (11,0.5) {$K_5$};
      \draw (12,1) -- (12,1.2);
      \node[anchor=mid] at (12,1.5) {384};
      \draw (12,1) -- (14,1) -- (14,0) -- (12,0);
      \node[anchor=mid] at (13,0.5) {$K_6$};
      \draw (14,1) -- (14,1.2);
      \node[anchor=mid] at (14,1.5) {448};
      \draw (14,1) -- (16,1) -- (16,0) -- (14,0);
      \node[anchor=mid] at (15,0.5) {$K_7$};
      \draw (16,1) -- (16,1.2);
      \node[anchor=mid] at (16,1.5) {512};

      \node[anchor=east] at (-0.5,-2) {payload vector};
      \node[anchor=mid] at (-0.8,-1) {\small bit};
      \draw (0,-1.5) -- (0,-1.3);
      \node[anchor=mid,rotate=40] at (0,-1) {0};
      \draw (0,-2.5) -- (0,-1.5) -- (1,-1.5) -- (1,-2.5) -- cycle;
      \node[anchor=mid] at (0.5,-2) {$P_0$};
      \draw (1,-1.5) -- (1,-1.3);
      \node[anchor=mid,rotate=40] at (1,-1) {32};
      \draw (1,-1.5) -- (2,-1.5) -- (2,-2.5) -- (1,-2.5);
      \node[anchor=mid] at (1.5,-2) {$P_1$};
      \draw (2,-1.5) -- (2,-1.3);
      \node[anchor=mid,rotate=40] at (2,-1) {64};
      \draw (2,-1.5) -- (3,-1.5) -- (3,-2.5) -- (2,-2.5);
      \node[anchor=mid] at (2.5,-2) {$P_2$};
      \draw (3,-1.5) -- (3,-1.3);
      \node[anchor=mid,rotate=40] at (3,-1) {96};
      \draw (3,-1.5) -- (4,-1.5) -- (4,-2.5) -- (3,-2.5);
      \node[anchor=mid] at (3.5,-2) {$P_3$};
      \draw (4,-1.5) -- (4,-1.3);
      \node[anchor=mid,rotate=40] at (4,-1) {128};
      \draw (4,-1.5) -- (5,-1.5) -- (5,-2.5) -- (4,-2.5);
      \node[anchor=mid] at (4.5,-2) {$P_4$};
      \draw (5,-1.5) -- (5,-1.3);
      \node[anchor=mid,rotate=40] at (5,-1) {160};
      \draw (5,-1.5) -- (6,-1.5) -- (6,-2.5) -- (5,-2.5);
      \node[anchor=mid] at (5.5,-2) {$P_5$};
      \draw (6,-1.5) -- (6,-1.3);
      \node[anchor=mid,rotate=40] at (6,-1) {192};
      \draw (6,-1.5) -- (7,-1.5) -- (7,-2.5) -- (6,-2.5);
      \node[anchor=mid] at (6.5,-2) {$P_6$};
      \draw (7,-1.5) -- (7,-1.3);
      \node[anchor=mid,rotate=40] at (7,-1) {224};
      \draw (7,-1.5) -- (8,-1.5) -- (8,-2.5) -- (7,-2.5);
      \node[anchor=mid] at (7.5,-2) {$P_7$};
      \draw (8,-1.5) -- (8,-1.3);
      \node[anchor=mid,rotate=40] at (8,-1) {256};
    \end{tikzpicture}
    \caption{Smaller payload (64-bit key, 32-bit payload)}
  \end{subfigure}\\
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}[yscale=0.75, xscale=0.625]
      \node[anchor=east] at (-0.5,0.5) {key vector};
      \node[anchor=mid] at (-0.8,1.5) {\small bit};
      \draw (0,1) -- (0,1.2);
      \node[anchor=mid,rotate=40] at (0,1.5) {0};
      \draw (0,0) -- (0,1) -- (1,1) -- (1,0) -- cycle;
      \node[anchor=mid] at (0.5,0.5) {$K_0$};
      \draw (1,1) -- (1,1.2);
      \node[anchor=mid,rotate=40] at (1,1.5) {32};
      \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
      \node[anchor=mid] at (1.5,0.5) {$K_1$};
      \draw (2,1) -- (2,1.2);
      \node[anchor=mid,rotate=40] at (2,1.5) {64};
      \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
      \node[anchor=mid] at (2.5,0.5) {$K_2$};
      \draw (3,1) -- (3,1.2);
      \node[anchor=mid,rotate=40] at (3,1.5) {96};
      \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
      \node[anchor=mid] at (3.5,0.5) {$K_3$};
      \draw (4,1) -- (4,1.2);
      \node[anchor=mid,rotate=40] at (4,1.5) {128};
      \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
      \node[anchor=mid] at (4.5,0.5) {$K_4$};
      \draw (5,1) -- (5,1.2);
      \node[anchor=mid,rotate=40] at (5,1.5) {160};
      \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
      \node[anchor=mid] at (5.5,0.5) {$K_5$};
      \draw (6,1) -- (6,1.2);
      \node[anchor=mid,rotate=40] at (6,1.5) {192};
      \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
      \node[anchor=mid] at (6.5,0.5) {$K_6$};
      \draw (7,1) -- (7,1.2);
      \node[anchor=mid,rotate=40] at (7,1.5) {224};
      \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
      \node[anchor=mid] at (7.5,0.5) {$K_7$};
      \draw (8,1) -- (8,1.2);
      \node[anchor=mid,rotate=40] at (8,1.5) {256};

      \node[anchor=east] at (-0.5,-2) {payload vector};
      \node[anchor=mid] at (-0.8,-1) {\small bit};
      \draw (0,-1.5) -- (0,-1.3);
      \node[anchor=mid] at (0,-1) {0};
      \draw (0,-2.5) -- (0,-1.5) -- (2,-1.5) -- (2,-2.5) -- cycle;
      \node[anchor=mid] at (1,-2) {$P_0$};
      \draw (2,-1.5) -- (2,-1.3);
      \node[anchor=mid] at (2,-1) {64};
      \draw (2,-2.5) -- (4,-2.5) -- (4,-1.5) -- (2,-1.5);
      \node[anchor=mid] at (3,-2) {$P_1$};
      \draw (4,-1.5) -- (4,-1.3);
      \node[anchor=mid] at (4,-1) {128};
      \draw (4,-2.5) -- (6,-2.5) -- (6,-1.5) -- (4,-1.5);
      \node[anchor=mid] at (5,-2) {$P_2$};
      \draw (6,-1.5) -- (6,-1.3);
      \node[anchor=mid] at (6,-1) {192};
      \draw (6,-2.5) -- (8,-2.5) -- (8,-1.5) -- (6,-1.5);
      \node[anchor=mid] at (7,-2) {$P_3$};
      \draw (8,-1.5) -- (8,-1.3);
      \node[anchor=mid] at (8,-1) {256};
      \draw (8,-2.5) -- (10,-2.5) -- (10,-1.5) -- (8,-1.5);
      \node[anchor=mid] at (9,-2) {$P_4$};
      \draw (10,-1.5) -- (10,-1.3);
      \node[anchor=mid] at (10,-1) {320};
      \draw (10,-2.5) -- (12,-2.5) -- (12,-1.5) -- (10,-1.5);
      \node[anchor=mid] at (11,-2) {$P_5$};
      \draw (12,-1.5) -- (12,-1.3);
      \node[anchor=mid] at (12,-1) {384};
      \draw (12,-2.5) -- (14,-2.5) -- (14,-1.5) -- (12,-1.5);
      \node[anchor=mid] at (13,-2) {$P_6$};
      \draw (14,-1.5) -- (14,-1.3);
      \node[anchor=mid] at (14,-1) {448};
      \draw (14,-2.5) -- (16,-2.5) -- (16,-1.5) -- (14,-1.5);
      \node[anchor=mid] at (15,-2) {$P_7$};
      \draw (16,-1.5) -- (16,-1.3);
      \node[anchor=mid] at (16,-1) {512};
    \end{tikzpicture}
    \caption{Smaller key (32-bit key, 64-bit payload)}
    \label{fig:register_usage_different_payload_larger}
  \end{subfigure}
  \caption{Use of smaller vector registers to avoid wasted space.}
  \label{fig:register_usage_different_payload}
\end{figure}

However, when there is a payload larger than the key, one might notice that
the vector register used for the keys is no longer the largest available one,
as can be seen in figure~\ref{fig:register_usage_different_payload_larger}.

To avoid this, a second option is proposed:
The second method always uses the largest available vector register for the keys.
However, that means that the payloads that are larger than the key
are loaded into a vector register which
is larger than the largest available vector register.
For this, the emulated vector registers of sizes 128, 256 and 512 bytes
(1024, 2048 and 4096 bits) introduced in section~\ref{sec:template_wrapper}
are used.
By emulating larger vector registers, more keys can be loaded into a single
vector register, as shown in
figure~\ref{fig:register_usage_double_payload_vec}, and thus
more elements can be sorted in parallel.
However, since the larger vector registers are emulated, multiple SIMD instructions
need to be executed to execute a single operation, which may negate the performance
gained by using the largest available (non-emulated) vector register for the keys.

In the following, the version of RadixSIMD that uses the first option
(that does not use the larger emulated vector registers)
is called \mbox{`RadixSIMDOneReg'}.
The version that uses the second option (that uses the larger emulated vector registers)
is simply called \mbox{`RadixSIMD'}.

Which one of these two possible options for handling datasets with larger payloads
than keys is faster is explored in chapter~\ref{chap:results}.

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[yscale=0.75, xscale=0.625]
    \node[anchor=east] at (-0.5,0.5) {key vector};
    \node[anchor=mid] at (-1,1.5) {\small bit};
    \draw (0,1) -- (0,1.2);
    \node[anchor=mid,rotate=40] at (0,1.5) {0};
    \draw (0,0) -- (0,1) -- (1,1) -- (1,0) -- cycle;
    \node[anchor=mid] at (0.5,0.5) {$K_0$};
    \draw (1,1) -- (1,1.2);
    \node[anchor=mid,rotate=40] at (1,1.5) {32};
    \draw (1,0) -- (2,0) -- (2,1) -- (1,1);
    \node[anchor=mid] at (1.5,0.5) {$K_1$};
    \draw (2,1) -- (2,1.2);
    \node[anchor=mid,rotate=40] at (2,1.5) {64};
    \draw (2,0) -- (3,0) -- (3,1) -- (2,1);
    \node[anchor=mid] at (2.5,0.5) {$K_2$};
    \draw (3,1) -- (3,1.2);
    \node[anchor=mid,rotate=40] at (3,1.5) {96};
    \draw (3,0) -- (4,0) -- (4,1) -- (3,1);
    \node[anchor=mid] at (3.5,0.5) {$K_3$};
    \draw (4,1) -- (4,1.2);
    \node[anchor=mid,rotate=40] at (4,1.5) {128};
    \draw (4,0) -- (5,0) -- (5,1) -- (4,1);
    \node[anchor=mid] at (4.5,0.5) {$K_4$};
    \draw (5,1) -- (5,1.2);
    \node[anchor=mid,rotate=40] at (5,1.5) {160};
    \draw (5,0) -- (6,0) -- (6,1) -- (5,1);
    \node[anchor=mid] at (5.5,0.5) {$K_5$};
    \draw (6,1) -- (6,1.2);
    \node[anchor=mid,rotate=40] at (6,1.5) {192};
    \draw (6,0) -- (7,0) -- (7,1) -- (6,1);
    \node[anchor=mid] at (6.5,0.5) {$K_6$};
    \draw (7,1) -- (7,1.2);
    \node[anchor=mid,rotate=40] at (7,1.5) {224};
    \draw (7,0) -- (8,0) -- (8,1) -- (7,1);
    \node[anchor=mid] at (7.5,0.5) {$K_7$};
    \draw (8,1) -- (8,1.2);
    \node[anchor=mid,rotate=40] at (8,1.5) {256};
    \draw (8,0) -- (9,0) -- (9,1) -- (8,1);
    \node[anchor=mid] at (8.5,0.5) {$K_8$};
    \draw (9,1) -- (9,1.2);
    \node[anchor=mid,rotate=40] at (9,1.5) {288};
    \draw (9,0) -- (10,0) -- (10,1) -- (9,1);
    \node[anchor=mid] at (9.5,0.5) {$K_9$};
    \draw (10,1) -- (10,1.2);
    \node[anchor=mid,rotate=40] at (10,1.5) {320};
    \draw (10,0) -- (11,0) -- (11,1) -- (10,1);
    \node[anchor=mid] at (10.5,0.5) {$K_{10}$};
    \draw (11,1) -- (11,1.2);
    \node[anchor=mid,rotate=40] at (11,1.5) {352};
    \draw (11,0) -- (12,0) -- (12,1) -- (11,1);
    \node[anchor=mid] at (11.5,0.5) {$K_{11}$};
    \draw (12,1) -- (12,1.2);
    \node[anchor=mid,rotate=40] at (12,1.5) {384};
    \draw (12,0) -- (13,0) -- (13,1) -- (12,1);
    \node[anchor=mid] at (12.5,0.5) {$K_{12}$};
    \draw (13,1) -- (13,1.2);
    \node[anchor=mid,rotate=40] at (13,1.5) {416};
    \draw (13,0) -- (14,0) -- (14,1) -- (13,1);
    \node[anchor=mid] at (13.5,0.5) {$K_{13}$};
    \draw (14,1) -- (14,1.2);
    \node[anchor=mid,rotate=40] at (14,1.5) {448};
    \draw (14,0) -- (15,0) -- (15,1) -- (14,1);
    \node[anchor=mid] at (14.5,0.5) {$K_{14}$};
    \draw (15,1) -- (15,1.2);
    \node[anchor=mid,rotate=40] at (15,1.5) {480};
    \draw (15,0) -- (16,0) -- (16,1) -- (15,1);
    \node[anchor=mid] at (15.5,0.5) {$K_{15}$};
    \draw (16,1) -- (16,1.2);
    \node[anchor=mid,rotate=40] at (16,1.5) {512};

    \node[anchor=east] at (-0.5,-2) {\shortstack[l]{first half of\\payload vector}};
    \node[anchor=mid] at (-1,-1) {\small bit};
    \draw (0,-1.5) -- (0,-1.3);
    \node[anchor=mid] at (0,-1) {0};
    \draw (0,-2.5) -- (0,-1.5) -- (2,-1.5) -- (2,-2.5) -- cycle;
    \node[anchor=mid] at (1,-2) {$P_0$};
    \draw (2,-1.5) -- (2,-1.3);
    \node[anchor=mid] at (2,-1) {64};
    \draw (2,-2.5) -- (4,-2.5) -- (4,-1.5) -- (2,-1.5);
    \node[anchor=mid] at (3,-2) {$P_1$};
    \draw (4,-1.5) -- (4,-1.3);
    \node[anchor=mid] at (4,-1) {128};
    \draw (4,-2.5) -- (6,-2.5) -- (6,-1.5) -- (4,-1.5);
    \node[anchor=mid] at (5,-2) {$P_2$};
    \draw (6,-1.5) -- (6,-1.3);
    \node[anchor=mid] at (6,-1) {192};
    \draw (6,-2.5) -- (8,-2.5) -- (8,-1.5) -- (6,-1.5);
    \node[anchor=mid] at (7,-2) {$P_3$};
    \draw (8,-1.5) -- (8,-1.3);
    \node[anchor=mid] at (8,-1) {256};
    \draw (8,-2.5) -- (10,-2.5) -- (10,-1.5) -- (8,-1.5);
    \node[anchor=mid] at (9,-2) {$P_4$};
    \draw (10,-1.5) -- (10,-1.3);
    \node[anchor=mid] at (10,-1) {320};
    \draw (10,-2.5) -- (12,-2.5) -- (12,-1.5) -- (10,-1.5);
    \node[anchor=mid] at (11,-2) {$P_5$};
    \draw (12,-1.5) -- (12,-1.3);
    \node[anchor=mid] at (12,-1) {384};
    \draw (12,-2.5) -- (14,-2.5) -- (14,-1.5) -- (12,-1.5);
    \node[anchor=mid] at (13,-2) {$P_6$};
    \draw (14,-1.5) -- (14,-1.3);
    \node[anchor=mid] at (14,-1) {448};
    \draw (14,-2.5) -- (16,-2.5) -- (16,-1.5) -- (14,-1.5);
    \node[anchor=mid] at (15,-2) {$P_7$};
    \draw (16,-1.5) -- (16,-1.3);
    \node[anchor=mid] at (16,-1) {512};

    \node[anchor=east] at (-0.5,-4.5) {\shortstack[l]{second half of\\payload vector}};
    \node[anchor=mid] at (-1,-3.5) {\small bit};
    \draw (0,-4) -- (0,-3.8);
    \node[anchor=mid] at (0,-3.5) {512};
    \draw (0,-5) -- (0,-4) -- (2,-4) -- (2,-5) -- cycle;
    \node[anchor=mid] at (1,-4.5) {$P_8$};
    \draw (2,-4) -- (2,-3.8);
    \node[anchor=mid] at (2,-3.5) {576};
    \draw (2,-5) -- (4,-5) -- (4,-4) -- (2,-4);
    \node[anchor=mid] at (3,-4.5) {$P_9$};
    \draw (4,-4) -- (4,-3.8);
    \node[anchor=mid] at (4,-3.5) {640};
    \draw (4,-5) -- (6,-5) -- (6,-4) -- (4,-4);
    \node[anchor=mid] at (5,-4.5) {$P_{10}$};
    \draw (6,-4) -- (6,-3.8);
    \node[anchor=mid] at (6,-3.5) {704};
    \draw (6,-5) -- (8,-5) -- (8,-4) -- (6,-4);
    \node[anchor=mid] at (7,-4.5) {$P_{11}$};
    \draw (8,-4) -- (8,-3.8);
    \node[anchor=mid] at (8,-3.5) {768};
    \draw (8,-5) -- (10,-5) -- (10,-4) -- (8,-4);
    \node[anchor=mid] at (9,-4.5) {$P_{12}$};
    \draw (10,-4) -- (10,-3.8);
    \node[anchor=mid] at (10,-3.5) {832};
    \draw (10,-5) -- (12,-5) -- (12,-4) -- (10,-4);
    \node[anchor=mid] at (11,-4.5) {$P_{13}$};
    \draw (12,-4) -- (12,-3.8);
    \node[anchor=mid] at (12,-3.5) {896};
    \draw (12,-5) -- (14,-5) -- (14,-4) -- (12,-4);
    \node[anchor=mid] at (13,-4.5) {$P_{14}$};
    \draw (14,-4) -- (14,-3.8);
    \node[anchor=mid] at (14,-3.5) {960};
    \draw (14,-5) -- (16,-5) -- (16,-4) -- (14,-4);
    \node[anchor=mid] at (15,-4.5) {$P_{15}$};
    \draw (16,-4) -- (16,-3.8);
    \node[anchor=mid] at (16,-3.5) {1024};
  \end{tikzpicture}
  \caption{Use of emulated, larger vector register for payloads to be able
    to use a full 512-bit AVX-512 vector register for the keys, when
    sorting a 32-bit key with a 64-bit payload.
    In this case, a 1024-bit vector register is emulated using two
    512-bit vector registers.}
  \label{fig:register_usage_double_payload_vec}
\end{figure}

To make the determination of the vector sizes easier, the parameter
\texttt{numElemsPerVec} is used. This parameter determines the number of
elements in each vector. The sizes of the vectors are then calculated
by multiplying this parameter with the size of the data type the vector
is supposed to hold.

Listing~\ref{lst:numElemsPerVec_impl} shows the C++ implementation of the
parameter \texttt{numElemsPerVec} as part of the \texttt{BitSorterSIMD} class.

\begin{listing}[h!]
  \begin{minted}[
    fontsize=\fontsize{10pt}{10pt},
    linenos,
    bgcolor=white,
    numbersep=5pt
    ]{c++}
template <bool OneReg = false> struct BitSorterSIMD {

  template <typename K, typename... Ps>
  static constexpr SortIndex
      numElemsPerVec = OneReg ? 64 / std::max({sizeof(K), sizeof(Ps)...})
                              : 64 / sizeof(K);

  //...
};
  \end{minted}
  \caption{Implementation of the parameter \texttt{numElemsPerVec} as a variable template
    as part of the \texttt{BitSorterSIMD} class.}
  \label{lst:numElemsPerVec_impl}
\end{listing}

For the first of the above mentioned options for choosing the vector register
sizes, which is used by the algorithm RadixSIMDOneReg, the parameter
\texttt{numElemsPerVec} is set to 64 divided by the size of the largest
data type in bytes. This leads to the largest available (non-emulated)
vector register being used for the largest data type and smaller
vector registers for smaller data types.

For the second of the options mentioned above, which is used by the
algorithm RadixSIMD, the parameter \texttt{numElemsPerVec} is set to
64 divided by the size of the key data type. This leads to the largest
available (non-emulated) vector register being used for the key data type
and different sized, potentially emulated, vector registers for payloads
of a different size.

\section{Comparison sorter for small subarrays}
\label{sec:comparision_sorter_small_subarrays_simd}

As mentioned in section~\ref{sec:comparison_sorter_for_small_subarrays_seq},
the MSB Radix Sort implementation developed for this thesis
switches to a comparison sorter for small subarrays.
The threshold \texttt{cmpSortThreshold} controls the size below
which the comparison sorter is used.

For the algorithms RadixSIMD and RadixSIMDOneReg, insertion sort is
used as the comparison sorter.

There are two further comparison sorters examined.

One of them is the small sort used by the SIMD Quicksort
implementation by \citet{bramas}, which is a bitonic sort.
It can sort arrays that fit into 16 512-bit vectors,
meaning arrays with a size of up to \texttt{16*64/sizeof(K)}
elements, where \texttt{sizeof(K)} is the size of the data type
used for the keys in bytes.
The algorithm using this small sort is called `RadixSIMDBramSmall' in
the following.

The other comparison sorter is not actually a sorting algorithm,
as it simply does nothing. So the algorithm using this comparison sorter,
which is called `RadixSIMDNoCmp' in the following, does not actually fully sort
the array.
Instead, it is used to provide an upper bound
for performance improvements that can be gained by just
optimizing the comparison sorter.

Additionally, even though RadixSIMDNoCmp does not fully
sort the array, it still sorts the array partly, meaning
if an array was sorted with the algorithm RadixSIMDNoCmp,
the elements in the array are guaranteed to be at most
\texttt{cmpSortThreshold} places away from the position
they would be in if the array was sorted.
There could be applications where this is enough and a fully
sorted array is not actually needed.
The parameter \texttt{cmpSortThreshold} would then be used
to control the `degree of sortedness', where a larger value
decreases computation time but also decreases the `degree of
sortedness'.

What value of \texttt{cmpSortThreshold} is optimal is explored in
section~\ref{sec:determining-the-optimal-comparison-sorter-threshold}.


\chapter{Experiments}\label{chap:experiments}

To evaluate the performance of the SIMD-Implementation of MSB Radix Sort developed
for this thesis, the runtime of this algorithm as well as other algorithms for
comparison was measured.

In order to develop a thorough understanding of the performance characteristics of the
algorithm in comparison to the other algorithms, the experiments were conducted
with different input sizes, different combinations of key and payload types
as well as different distributions of the input data.

\section{Key and Payload Data Types}

The following data types are used for testing:
\begin{itemize}
  \setlength\itemsep{0em}
  \item \texttt{int8}: 8-bit signed integer
  \item \texttt{int16}: 16-bit signed integer
  \item \texttt{int32}: 32-bit signed integer
  \item \texttt{int64}: 64-bit signed integer
  \item \texttt{float}: 32-bit floating point number
  \item \texttt{double}: 64-bit floating point number
\end{itemize}

These types are then used to form different combinations of key and payload data
types with different numbers of payloads. These are written in the form
`\texttt{keyType-payloadType1-payloadType2-...}'. For example, the key payload
data type combination where the key is a \texttt{float} and the payloads are
an \texttt{int32} and an \texttt{int64} is written as \texttt{float-int32-int64}.

Note that unsigned integer types are not tested. This is because when sorting,
they only differ in the direction the highest bit (the sign) is sorted and thus
almost certainly do not provide additional insight.


\section{Tested sorting algorithms}
\label{sec:tested-sorting-algorithms}

The following algorithms were analyzed:

\begin{itemize}
  \item \textbf{Radix:} The MSB Radix Sort implementation developed for this thesis.
        There are multiple versions analyzed:
        \begin{itemize}
          \item \textbf{RadixSeq:} The sequential version of the algorithm.
          \item \textbf{RadixSIMD:} The SIMD version of the algorithm, using the
                second option introduced in section~\ref{sec:choosing_numElemsPerVec}
                (always using a 512-bit AVX-512 vector register for the keys).
          \item \textbf{RadixSIMDOneReg:} The SIMD version of the algorithm, using the
                first option introduced in section~\ref{sec:choosing_numElemsPerVec}
                (using a smaller vector register for key/payload data types that are
                smaller than the largest key/payload data type).
          \item \textbf{RadixSIMDBramSmall:} RadixSIMD, but using
                the small sort algorithm developed by \cite{bramas},
                which uses bitonic sort instead of insertion sort.
          \item \textbf{RadixSIMDNoCmp:} RadixSIMD, but
                with a pseudo comparison sorter which does nothing.
                This version does not actually completely sort the data,
                but is used to provide an upper bound for the performance improvements
                that could be achieved by just optimizing the comparison sorter.
        \end{itemize}
  \item \textbf{Moeller:}
        \begin{itemize}
          \item \textbf{MoellerSeq:} The sequential implementation of MSB Radix Sort
                developed by \cite{moeller_radix}.
          \item \textbf{MoellerCompress:} The SIMD implementation of MSB Radix Sort
                using AVX-512 \texttt{mask\_compressstoreu} instruction developed by \cite{moeller_radix}.
        \end{itemize}
  \item \textbf{STLSort:} The sorting algorithm included in the C++ standard
        template library (STL) \citep{enwiki:stl_sort}.
  \item \textbf{IPPRadix:} The Radix Sort implementation included in the Intel
        Performance Primitives library \citep{intel_ipp}.
  \item \textbf{BramasSort:} The AVX-512 Quicksort implementation developed by \cite{bramas}.
  \item \textbf{BlacherSort:} The AVX2 Quicksort implementation developed by \cite{blacher}.
\end{itemize}


Note that some of the sorting algorithms analyzed do not support
all key and payload data type combinations. MoellerSeq and MoellerCompress
only support up to one payload which has to have the same size as the key.
\mbox{IPPRadix} for some reason does not support any payload data type and the key data type
cannot be \texttt{int8} (however all other key data types are supported, including
\texttt{uint8}). BramasSort only supports the combinations
\texttt{int32}, \texttt{double} and \texttt{int32-int32}. BlacherSort
only supports the combination \texttt{int32}. RadixSIMDBramSmall
supports the same combinations as BramasSort, so it supports only the combinations
\texttt{int32}, \texttt{double} and \texttt{int32-int32}.

\section{Input data}
\label{sec:input-data}

The input data for each test is randomly generated. The keys are generated
using different distributions while the payloads were randomly generated
using the corresponding key as a seed to make checking the payloads later more
easy.

The distributions used for generating the keys are:

\begin{itemize}
  \item \textbf{Uniform:} A uniform distribution. For integer key data types
        a uniform distribution over the whole range of the integer is used.
        For floating point key data types a uniform distribution in $[-1,1[$ was used.
  \item \textbf{Gaussian:} A gaussian distribution with
        mean $\mu=0$. For floating point data types a standard deviation of $\sigma=1$
        was used and for integer data types a standard deviation of $\sigma=100$
        was used. For integer data types the distribution is generated from a
        gaussian distribution of \texttt{double}s and then rounded to the
        nearest integer.
  \item \textbf{Zero:} All keys of the input data are zero.
  \item \textbf{ZeroOne:} All keys of the input data are either zero or one,
        with equal probability. This provides a distribution with only two different
        values but with high entropy.
  \item \textbf{Sorted:} An already in ascending order sorted dataset. This
        is generated by first generating a Uniform distribution for integers and
        a Gaussian distribution for floating point numbers and then sorting the data.
  \item \textbf{ReverseSorted:} The same as the Sorted distribution but in reverse order.
  \item \textbf{AlmostSorted:} The same as the Sorted distribution, but with
        $\lfloor2^{\log_{10}(n)}\rfloor$ pairs of elements swapped, where $n$ is the number of elements.
  \item \textbf{AlmostReverseSorted:} The same as the ReverseSorted distribution, but with
        $\lfloor2^{\log_{10}(n)}\rfloor$ pairs of elements swapped, where $n$ is the number of elements.
\end{itemize}

\section{Test environment}
The tests are run on a machine with an Intel© Core™ i7-12700K CPU running at 5 GHz
and 32 GB of main memory. The operating system used for
the tests is Manjaro Linux with kernel version 5.17.15.
The code was compiled with gcc version 12.1.0.

The processor provides the following AVX-512 instruction
set extensions: \mbox{AVX-512F}, \mbox{AVX-512DQ}, \mbox{AVX-512BW}, \mbox{AVX-512IFMA}, \mbox{AVX-512CD}, \mbox{AVX-512BW},
\mbox{AVX-512VL}, \mbox{AVX-512BF16}, \mbox{AVX-512VBMI}, \mbox{AVX-512VBMI2}, \mbox{AVX-512VNNI}, \mbox{AVX-512BITALG},
\mbox{AVX-512VPOPCNTDQ},\\
\mbox{AVX-512VP2INTERSECT} and \mbox{AVX-512FP16}.

Note that Intel does not officially support AVX-512 on Alder Lake processors
(which includes the Intel© Core™ i7-12700K processor) and fused off the AVX-512
circuitry entirely a few months after the processor family was released
\citep{avx-512-fused-off}.
AVX-512 had to be enabled for the Intel© Core™ i7-12700K processor in the BIOS
to run the tests for this thesis.
This should, however, not have an effect on the results of the tests.


\section{Time measurements}

The sorting time of each algorithm is measured by a call to \texttt{clock\_gettime}
with the \texttt{CLOCK\_PROCESS\_CPUTIME\_ID} clock id
right before and after the call to the sorting function.
Since some of the sorting algorithms that support payloads require the data
in Array of Structures (AoS) layout, and some require the data in Structure of
Arrays (SoA) layout, the data is converted to the required layout before
the sorting is performed. This conversion is not included in the time measurement.

To ensure reliable results even for small datasets, the sorting is performed
multiple times and the average time is taken. The number of runs is
$\max\left(1,\lfloor\frac{2^{22}}{n}\rfloor\right)$ where $n$ is the number of
elements in the input data.
Additionally, to avoid potential warmup effects, warmup runs are performed before the actual
measurement. The number of warmup runs is
$\max\left(1,\lfloor\frac{2^{18}}{n}\rfloor\right)$ where $n$ is the number of
elements in the input data.

For each run, a new input data set is generated in a new memory location to
avoid cache effects.

\section{Checks}

After each sorting run there are checks performed confirming that the data is actually
sorted. It is not checked whether the elements that are in the array
before the sorting are the same as the elements that are in the array after
the sorting. The payloads are not checked either. This is because these
additional checks have a runtime complexity of at least $\mathcal{O}(n^2)$
and thus would increase the time needed to perform the time measurements
significantly.

However, there was a dedicated test program written for the algorithms developed
for this thesis (RadixSeq, RadixSIMD, RadixSIMDOneReg and RadixSIMDBramSmall),
which tests many key payload data type combinations for all the input data distributions
with different dataset sizes.
It checks that the data is sorted, that each payload is associated with the correct key, that
there is no key missing and that there is no new key introduced by the
sorting algorithm. The number of duplicate keys is not checked, meaning
that the sorting algorithm could hypothetically overwrite one instance of a duplicate key
with another key without the checks noticing this.
However if the sorting algorithm is actually incorrect, this is very unlikely
to happen only to duplicate keys and thus the checks would almost certainly
catch the error in the algorithm.


\chapter{Results}
\label{chap:results}

\section{Determining the optimal comparison sorter threshold}
\label{sec:determining-the-optimal-comparison-sorter-threshold}

The MSB Radix Sort algorithm developed in this thesis as well as the
implementation by \cite{moeller_radix} switch to a comparison sorter
when the number of elements to be sorted is below the threshold
\texttt{cmpSortThreshold}, as described in
section~\ref{sec:comparison_sorter_for_small_subarrays_seq}
and \ref{sec:comparision_sorter_small_subarrays_simd}.

It is very difficult to determine the threshold by just analyzing the
code of the algorithm because it depends on many non-obvious factors.
So the threshold is
determined experimentally by running the algorithm
with multiple different thresholds and multiple different array sizes
and then comparing the results.
This is done for different key and payload data types.

The optimal comparison sorter threshold is only determined for the
distributions Uniform and Gaussian.

However, only the data for the  distribution Uniform is shown, since
for floating point key types, the runtimes for Uniform and Gaussian
distributions are very similar and did not seem to affect the optimal
threshold at all and for integer key types, the threshold did not seem to affect
the runtime when sorting with Gaussian distributions.
%This is likely
%because the standard deviation of $\sigma=100$ results in many duplicate values
%in the input data, which results in the subarrays of the MSB Radix Sort algorithm

Additionally, only the data where the array size
was $2^{18}=262144$ is shown, since the size of the array did not seem to
have an impact on which threshold was optimal either.

For the RadixSIMDOneReg algorithm, only key payload data type
combinations were tested, where RadixSIMDOneReg actually differs from
RadixSIMD, i.e. only where there is a payload data type that is larger
than the key data type.

Also, the RadixSIMDBramSmall algorithm is only tested for the key
payload data type combinations that are supported, which are
\texttt{int32}, \texttt{double} and \texttt{int32-int32}.



Figures~\ref{fig:cmpThresh-MoellerSeq}, \ref{fig:cmpThresh-RadixSeq},
\ref{fig:cmpThresh-MoellerCompress}, \ref{fig:cmpThresh-RadixSIMD},
\ref{fig:cmpThresh-RadixSIMDOneReg} and \ref{fig:cmpThresh-RadixSIMDBramSmall}
show the runtime of MoellerSeq, RadixSeq, MoellerCompress, RadixSIMD,
RadixSIMDOneReg and RadixSIMDBramSmall
respectively, for different thresholds and key payload data type combinations.

For all tested algorithms there is a clear optimum visible.

For the two sequential algorithms MoellerSeq and RadixSeq, a threshold of $2^6=64$ seems
to be a reasonable choice and for the algorithms MoellerCompress, RadixSIMD and
RadixSIMDOneReg, a threshold of $2^4=16$ seems to be acceptable.

For the algorithm RadixSIMDBramSmall, the performance seems to be better the
larger the threshold is.
As mentioned in section~\ref{sec:comparision_sorter_small_subarrays_simd},
the small sort algorithm by \citet{bramas} supports arrays with a size of
up to \texttt{16*64/sizeof(K)}
elements, where \texttt{sizeof(K)} is the size of the data type
used for the keys in bytes. For the key data type \texttt{int32}
this is $16*64/4=256=2^{8}$ elements.
So, for the algorithm RadixSIMDBramSmall, the threshold is chosen
as large as possible while not exceeding the above mentioned limit,
i.e. it is set to \texttt{16*64/sizeof(K)}.
This however means that the sorting tree of RadixSIMDBramSmall
is much shallower than the sorting trees of the other algorithms,
possibly affecting the comparability.

The above mentioned values for \texttt{cmpSortThreshold} are used in the
following tests. For the algorithm RadixSIMDNoCmp the same value as
for RadixSIMD is used.

% https://tex.stackexchange.com/questions/5769/two-figures-side-by-side
\begin{figure}[H]
  \centering
  \threshMinipage{MoellerSeq}{82}{south}\hfill
  \threshMinipage{RadixSeq}{75}{south}
\end{figure}
\begin{figure}[H]
  \centering
  \threshMinipage{MoellerCompress}{70}{north}\hfill
  \threshMinipage{RadixSIMD}{55}{north}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.49\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
          ylabel={Time per element [ns]},
          xlabel={Threshold},
          height=0.4\textheight,
          width=\textwidth,
          xminorgrids=true,
          xmajorgrids=true,
          yminorgrids=true,
          ymajorgrids=true,
          xmode=log,
          log basis x=2,
          %xmin=2,
          ymax=40,
          xtick={2^1,2^2,2^3,2^4,2^5,2^6,2^7,2^8,2^9},
          legend style={fill=white, fill opacity=0.6, draw opacity=1,text opacity=1,font=\footnotesize},
          legend cell align={left},
          legend pos= north west,
        ]

        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-float-int64-RadixSIMDOneReg-Uniform.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int8-int64-RadixSIMDOneReg-Uniform.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int16-int64-RadixSIMDOneReg-Uniform.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int32-int64-RadixSIMDOneReg-Uniform.dat};

        \legend{\texttt{float-int64}, \texttt{int8-int64}, \texttt{int16-int64}, \texttt{int32-int64}}
      \end{axis}
    \end{tikzpicture}
    \caption{Runtime of RadixSIMD\-OneReg for different values for \texttt{cmpSortThreshold}, $2^{18}$ elements, Uniform distribution}
    \label{fig:cmpThresh-RadixSIMDOneReg}
  \end{minipage}
  \hfill
  \begin{minipage}{0.49\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
          ylabel={Time per element [ns]},
          xlabel={Threshold},
          height=0.4\textheight,
          width=\textwidth,
          xminorgrids=true,
          xmajorgrids=true,
          yminorgrids=true,
          ymajorgrids=true,
          xmode=log,
          log basis x=2,
          %xmin=2,
          ymax=35,
          xtick={2^1,2^2,2^3,2^4,2^5,2^6,2^7,2^8,2^9},
          legend style={fill=white, fill opacity=0.6, draw opacity=1,text opacity=1,font=\footnotesize},
          legend cell align={left},
          legend pos= north west,
        ]

        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-double-RadixSIMDBramSmall-Uniform.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int32-RadixSIMDBramSmall-Uniform.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int32-int32-RadixSIMDBramSmall-Uniform.dat};

        \legend{\texttt{double}, \texttt{int32}, \texttt{int32-int32}}
      \end{axis}
    \end{tikzpicture}
    \caption{Runtime of RadixSIMDBramSmall for different values for \texttt{cmpSortThreshold}, $2^{18}$ elements, Uniform distribution}
    \label{fig:cmpThresh-RadixSIMDBramSmall}
  \end{minipage}
\end{figure}

\section{Comparison between RadixSIMD and MoellerCompress}
\label{sec:cmpRadixSIMD-MoellerCompress}

One of the main goals of this thesis was a more efficient payload handling by
separating key and payload datastreams. To evaluate whether this goal was
achieved, RadixSIMD is compared to MoellerCompress.

However, there are also other differences between RadixSIMD and MoellerCompress
besides the way
payloads are handled and thus any performance difference between the two algorithms
may not be due to the difference in payload handling.
Therefore the algorithms are compared without payloads and with
payloads where the comparison without payloads provides a baseline and the comparison
with payloads can then be used to determine the performance impact of the
different payload handling.

The comparison is done for the key data types \texttt{float}, \texttt{double},
\texttt{int8}, \texttt{int16}, \texttt{int32} and \texttt{int64}. For each
key data type, the comparison is done for four different key payload
data type combinations: without any payload, with payload of the same as the
size of the key data type, with payload with half the size of the key data type
and
with payload with a quarter of the size of the key data type
(in this section, these combinations are called `without payload',
`with payload', `with half payload' and `with quarter payload', respectively).
For example for a \texttt{float} key data type, the comparison is done
for the combinations \texttt{float} (`without payload'), \texttt{float-int32}
(`with payload'), \texttt{float-int16} (`with half payload') and
\texttt{float-int8} (`with quarter payload').
For tests of MoellerCompress where the payload is
smaller than the key, the payload is padded to the size of the key data type.
Also, only combinations where a required data type for the payload exists
are tested. For example when the key data type is \texttt{int8}, there
does not exist a data type of half or quarter the size of the key data type.

The comparisons are done for all distributions introduced in
section~\ref{sec:input-data}.

The number of elements is $2^{18}=262144$ for all tests in this section.

\newcommand{\speedupFig}[4]{
  \begin{minipage}{\linewidth}
    \centering
    \captionsetup{type=figure}
    \begin{tikzpicture}
      \begin{axis}[xbar,
          symbolic y coords={
              double,
              float,
              int64,
              int32,
              int16,
              int8,
            },
          xbar=-2pt, bar width=-10pt,
          y dir=reverse,
          width=0.55\textwidth,
          height=0.65\textheight,
          xlabel style={align=center},
          ylabel=Speedup of #2 over #1,
          ylabel=Key data type,
          %xticklabel style={rotate=30,anchor=east},
          xminorgrids=true,
          xmajorgrids=true,
          xmin=0,
          xmax=#4,
          nodes near coords,
          nodes near coords align={horizontal},
          %bar width=5,
          %xtick={
          %    float,
          %    double,
          %    int8,
          %    int16,
          %    int32,
          %    int64
          %  },
          ytick=data,
          legend pos=outer north east,
          legend style={fill=white, fill opacity=0.6, draw opacity=1,text opacity=1},
          legend cell align={left},
          %cycle list name=withDashed,
        ]
        \addplot table[y=key_payloads, x=speedup] {data/relTo#1-#2-withoutPayload-#3-262144.dat};
        \addplot table[y=key_payloads, x=speedup] {data/relTo#1-#2-withPayloadFactor1-#3-262144.dat};
        \addplot table[y=key_payloads, x=speedup] {data/relTo#1-#2-withPayloadFactor2-#3-262144.dat};
        \addplot table[y=key_payloads, x=speedup] {data/relTo#1-#2-withPayloadFactor4-#3-262144.dat};
        %\addplot table[y=key_payloads, x=speedup] {data/relTo#1-#2-withPayloadFactor8-#3-262144.dat};
        \addplot[red,sharp plot,dashed,update limits=false,line legend] coordinates { (1,[normalized]-1) (1,[normalized]100) };
        \legend{without payload, with payload, with half payload, with quarter payload, no speedup}
      \end{axis}
    \end{tikzpicture}
    \captionof{figure}{Speedup of #2 over #1, #3 distribution}
    \label{fig:speedupRelTo-#1-#2-#3}
  \end{minipage}
}
\newcommand{\speedupFigOneReg}[1]{
  \begin{figure}[ht!]
    \centering
    \captionsetup{type=figure}
    \begin{tikzpicture}
      \begin{axis}[ybar,
          symbolic x coords={
              float-int64,
              int8-int64,
              int16-int64,
              int32-int64
            },
          width=0.7\textwidth,
          height=0.3\textheight,
          ylabel style={align=center},
          ylabel=Speedup of RadixSIMDOneReg\\over RadixSIMD,
          xlabel=Key and payload data type combination,
          %xticklabel style={rotate=30,anchor=east},
          yminorgrids=true,
          ymajorgrids=true,
          ymin=0,
          ymax=1.3,
          nodes near coords,
          %bar width=[normalized]5,
          xtick={
              float-int64,
              int8-int64,
              int16-int64,
              int32-int64
            },
          legend pos= north east,
          legend style={fill=white, fill opacity=0.6, draw opacity=1,text opacity=1},
          legend cell align={left},
          %cycle list name=withDashed,
        ]
        \addplot table[x=key_payloads, y=speedup] {data/relToRadixSIMD-RadixSIMDOneReg-withPayloadFactor1-#1-262144.dat};
        \addplot[red,sharp plot,dashed,update limits=false,line legend] coordinates { ([normalized]-1,1) ([normalized]100,1) };
        \legend{,no speedup}
      \end{axis}
    \end{tikzpicture}
    \caption{Speedup of RadixSIMDOneReg over RadixSIMD, $2^{18}$ elements, #1 distribution}
    \label{fig:speedupOneReg-#1}
  \end{figure}
}

\subsubsection*{Gaussian distribution}

\speedupFig{MoellerCompress}{RadixSIMD}{Gaussian}{2.2}

Figure~\ref{fig:speedupRelTo-MoellerCompress-RadixSIMD-Gaussian} shows the speedup of RadixSIMD
over MoellerCompress with the Gaussian distribution. For almost all key payload
data type combinations, RadixSIMD is faster than MoellerCompress up to a speedup
of about $1.8$. The speedup is
greater with smaller payloads and most of the time it is greater for
combinations with payload than for combinations without payload.

The exceptions are the key data type
\texttt{int64}, where the speedup is slightly less than
1 without payload and less than 1 when the payload has the same size as the key
and \texttt{int16} and \texttt{int8}, where the speedup for the
different combinations is very similar.

\subsubsection*{Uniform distribution}

\speedupFig{MoellerCompress}{RadixSIMD}{Uniform}{2.3}

Figure~\ref{fig:speedupRelTo-MoellerCompress-RadixSIMD-Uniform} shows the speedup of RadixSIMD
over MoellerCompress with the Uniform distribution. For this distribution,
RadixSIMD is faster than MoellerCompress for all key data types up to a
speedup of $1.94$.
Similar to the Gaussian distribution, for datasets with a payload, the
speedup of RadixSIMD over MoellerCompress is greater than without a payload
for all key data types except for \texttt{int16} and \texttt{int8}.
Also there is a slight increase in speedup as the payload gets smaller, however
it is not as pronounced as for the Gaussian distribution. For example for the
key data types \texttt{int64} and \texttt{int32}, the speedup is approximately
equal for all tested payload sizes.

\subsubsection*{Zero distribution}

\speedupFig{MoellerCompress}{RadixSIMD}{Zero}{1.7}

Figure~\ref{fig:speedupRelTo-MoellerCompress-RadixSIMD-Zero} shows the speedup of RadixSIMD
over MoellerCompress with the Zero distribution. Here, RadixSIMD is almost
exactly as fast as MoellerCompress for all key data types without payload.
With payload however, RadixSIMD is only half as fast as MoellerCompress
for the key data types \texttt{double}, \texttt{float}, \texttt{int64},
and \texttt{int32}. The speedup is higher for the key data types \texttt{int16}
and \texttt{int8}, but still below 1.
With half payload, the speedup is sometimes above 1 and sometimes below 1.
With quarter payload, the speedup is always between $1.2$ and $1.4$.

There currently is no explanation for this behavior.

\subsubsection*{ZeroOne distribution}

\speedupFig{MoellerCompress}{RadixSIMD}{ZeroOne}{2.5}

Figure~\ref{fig:speedupRelTo-MoellerCompress-RadixSIMD-ZeroOne} shows the speedup of RadixSIMD
over MoellerCompress with the ZeroOne distribution. The behavior
is very similar to the Zero distribution, with the biggest difference being
that the speedup for the key data types \texttt{double} with quarter payload is
$2$ instead of $1.2$.
As with the Zero distribution, this behavior can currently not be explained.

\subsubsection*{Sorted distribution}

\speedupFig{MoellerCompress}{RadixSIMD}{Sorted}{2.5}

Figure~\ref{fig:speedupRelTo-MoellerCompress-RadixSIMD-Sorted} shows the speedup of RadixSIMD
over MoellerCompress with the Sorted distribution.
In this case, RadixSIMD is
faster than MoellerCompress for all key data types except \texttt{int8}, where
the speedup is less than 1 with payload.
There is no clear pattern as to how the speedup depends on the size
or existence of a payload.

\subsubsection*{ReverseSorted distribution}

\speedupFig{MoellerCompress}{RadixSIMD}{ReverseSorted}{2.1}

Figure~\ref{fig:speedupRelTo-MoellerCompress-RadixSIMD-ReverseSorted} shows the speedup of RadixSIMD
over MoellerCompress with the ReverseSorted distribution.
The behavior
is very similar to the Sorted distribution. RadixSIMD is
faster than MoellerCompress for all key data types with and without payload except
for \texttt{int8}, with no pattern visible.
Why the fact that the data is reversed compared to the Sorted distribution
does not have an impact on the speedup is not clear.

\subsubsection*{AlmostSorted distribution}

\speedupFig{MoellerCompress}{RadixSIMD}{AlmostSorted}{2.4}

Figure~\ref{fig:speedupRelTo-MoellerCompress-RadixSIMD-AlmostSorted} shows the speedup of RadixSIMD
over MoellerCompress with the AlmostSorted distribution.
The graph for this distribution is almost identical to the graph
for the Sorted and ReverseSorted distribution.

This is probably because to generate the AlmostSorted distribution,
the Sorted distribution is used, as explained in section~\ref{sec:input-data}.
Why the fact that the data from the AlmostSorted distribution is
almost sorted instead of completely sorted does not have an impact on the
speedup, is not clear however.

\subsubsection*{AlmostReverseSorted distribution}

\speedupFig{MoellerCompress}{RadixSIMD}{AlmostReverseSorted}{2.2}

Figure~\ref{fig:speedupRelTo-MoellerCompress-RadixSIMD-AlmostReverseSorted} shows the speedup of RadixSIMD
over MoellerCompress with the AlmostReverseSorted distribution.
Similar to the AlmostSorted distribution, the graph for this distribution is
almost identical to the graph for the Sorted and ReverseSorted distribution.
The reason for this is not clear in this case either.

\section{Comparison between RadixSIMDOneReg and RadixSIMD}
\label{sec:comparingRadixSIMDAndRadixSIMDOneReg}

For handling key payload data type combinations where there is a payload data
type larger than the key data type, there are two methods
proposed in chapter~\ref{sec:choosing_numElemsPerVec}. One is used by RadixSIMD
and the other is used by RadixSIMDOneReg.

Since the results for different distributions are very similar, only the
results for the Uniform distribution are shown in figure~\ref{fig:speedupOneReg-Uniform}.

\speedupFigOneReg{Uniform}

RadixSIMDOneReg is slower than RadixSIMD for all key data
types. Also, RadixSIMDOneReg gets slower in comparison
to RadixSIMD as the key data type size decreases. The reason for this is
probably that RadixSIMD fills the largest available SIMD register completely
with keys and thus sorts more elements simultaneously as the size of the key
data type decreases whereas RadixSIMDOneReg always only sorts in this case 8
elements at a time.

So the emulation of larger SIMD vectors described in
section~\ref{sec:template_wrapper} does provide a performance benefit
compared to avoiding the emulation and sorting less elements at a time.

\section{Comparison with other sorting algorithms}
\label{sec:comparison-other-sorting-algorithms}

For the comparison with other sorting algorithms, all algorithms
were measured sorting a dataset with $2^{18}$ elements and the time
per element is shown. This was done for all distributions and several
key payload data type combinations.

Only some of the comparisons are shown in this section,
the comparison graphs for all distributions and all tested key payload data type combinations
can be found in appendix~\ref{appendix:graphs}.

Note that, as mentioned in section~\ref{sec:tested-sorting-algorithms},
not all tested algorithms support all key
payload data type combinations and are therefore not shown in all of the graphs.

The bars in the following figures
(\ref{fig:tpeBar-Uniform-int32}-\ref{fig:tpeBar-Zero-double})
are colored to make the graphs easier to read.
The bars for the sequential algorithms are colored black,
RadixSIMD is colored red and
RadixSIMDBramSmall is colored brown.
The bars for the rest of the algorithms are colored blue.
The bar for RadixSIMDNoCmp is dashed as a reminder that it does not
actually fully sort the data.

\tpeBarxmax{Uniform}{int32}{}{70}

Figure~\ref{fig:tpeBar-Uniform-int32} shows the time per element for
the Uniform distribution and the key payload data type combination
\texttt{int32} for different sorting algorithms.
The sequential sorting algorithms MoellerSeq, RadixSeq and STLSort are
the slowest with approximately $42$ to $57$ nanoseconds per element.
MoellerCompress is significantly
faster than the sequential algorithms, the runtime is approximately $15$
nanoseconds per element.
RadixSIMD is faster than MoellerCompress, with about two thirds of the runtime,
but slower than IPPRadix, BramasSort and BlacherSort.
BlacherSort is the fastest of the tested sorting algorithms, with a runtime
of $2.45$ nanoseconds per element.
The algorithms RadixSIMDBramSmall and RadixSIMDNoCmp are with $3.87$
and $3.98$ nanoseconds per
element only slightly slower than BramasSort.
This indicates that the RadixSIMD algorithm spends most of its time in the
comparison sorter and can thus be accelerated significantly by improving
the comparison sorter.


\tpeBarxmax{Uniform}{int32-int32}{}{70}

Figure~\ref{fig:tpeBar-Uniform-int32-int32} shows the time per element for
the Uniform distribution and the key payload data type combination
\texttt{int32-int32} for different sorting algorithms. The results are very
similar to the ones for \texttt{int32}. The runtimes for the sequential
algorithms are within $3\%$ of the ones for \texttt{int32}.
MoellerCompress is again significantly faster than the sequential algorithms
and RadixSIMD is almost $40\%$ faster than MoellerCompress.
BramasSort and RadixSIMDBramSmall again have an almost equal runtime of
approximately $5.3$ nanoseconds per element.
In this case BramasSort and RadixSIMDBramSmall are the fastest of the tested
sorting algorithms, which might only be due to the fact that BlacherSort
only supports the \texttt{int32} combination.
RadixSIMDNoCmp is slightly faster than RadixSIMDBramSmall and BramasSort.
This again indicates that the RadixSIMD algorithm spends most of its time in
the comparison sorter and can be accelerated significantly by improving
the comparison sorter.

\tpeBarxmax{Gaussian}{double}{}{70}

Figure~\ref{fig:tpeBar-Gaussian-double} shows the time per element for
the Gaussian distribution and the key payload data type combination
\texttt{double} for different sorting algorithms. The results are similar
to the ones for \texttt{int32} and \texttt{int32-int32} for the Uniform
distribution. The sequential algorithms MoellerSeq, RadixSeq and STLSort are
again the slowest. MoellerCompress is significantly faster than the sequential
algorithms and RadixSIMD is faster than MoellerCompress but does not
beat IPPRadix and BramasSort.


\tpeBarxmax{Zero}{int32}{}{8}

Figure~\ref{fig:tpeBar-Zero-int32} shows the time per element for the Zero
distribution and the key payload data type combination \texttt{int32} for
different sorting algorithms. BlacherSort is the fastest of the tested
sorting algorithms, in this case it is more than an order of magnitude faster
than the next fastest sorting algorithm. This is probably because BlacherSort
keeps track of the smallest and largest value while partitioning a subarray
enabling it to terminate early when the smallest and largest value are equal
and thus the subarray does not need to be sorted further.

The next fastest algorithms for this distribution are RadixSIMD,
RadixSIMDBramSmall, RadixSIMDNoCmp and MoellerCompress, which are all
almost exactly equally fast. STLSort is a bit slower
and IPPRadix, MoellerSeq and RadixSeq are almost twice as slow.

Interestingly, IPPRadix is almost exactly as fast as RadixSeq and MoellerSeq.

Very noticeable in this graph is the runtime of BramasSort, which is
several orders of magnitude slower than the other algorithms.
This is because BramasSort uses Quicksort without any early termination,
which for constant input leads to a `fully unbalanced' sorting tree
(i.e. every non-leaf node has exactly one child), leading to a
very deep recursion depth and thus a very slow runtime.

For this data type combination, the graph for the ZeroOne distribution
is very similar.


\tpeBarxmax{Zero}{double}{}{19}

Figure~\ref{fig:tpeBar-Zero-double} shows the time per element for the Zero
distribution and the key payload data type combination \texttt{double} for
different sorting algorithms. One might expect the results in this
case to be similar to the ones for \texttt{int32}, since in both
cases the array is filled with exactly the same content (apart
from the size of the key, which however should not change the relative
runtimes much).

However, the results are completely different. For
this data type combination, STLSort is the fastest of the tested sorting
algorithms by over a factor of $2$ and the sequential algorithms
RadixSeq and MoellerSeq are slightly faster than most of the SIMD
algorithms.

The reason for this is currently not clear, but it might somehow be
caused by a difference in how integers and floating point numbers are
compared.

Note that BlacherSort does not support the \texttt{double}
combination and thus cannot be shown in this graph, otherwise it might be the
fastest of the tested sorting algorithms in this case too.\\

\subsubsection{Summary of the comparisons with other sorting algorithms}

As mentioned above, more comparison graphs can be found in
appendix~\ref{appendix:graphs}.

Most of the time,
RadixSIMD is faster than MoellerCompress, except
for the distributions Zero and ZeroOne for almost
all key payload data type combinations. This was already seen in
section~\ref{sec:cmpRadixSIMD-MoellerCompress}.

RadixSIMD is also faster than the sequential algorithms
except for the distributions Zero, ZeroOne, Sorted
and ReverseSorted most of the time.
This is probably due to the fact that RadixSIMD
stores every element that was read from memory
back to memory, whereas the sequential algorithms
only write the elements back to memory when necessary, which
for example for the Zero distribution is never the case.

The algorithms RadixSIMDNoCmp and RadixSIMDBramSmall are
almost always significantly faster than RadixSIMD,
indicating that the RadixSIMD algorithm spends most of the sorting time in the
comparison sorter. Thus, by improving the comparison sorter,
there could be significant speedup achieved.

The algorithm BlacherSort is the fastest
or only slightly slower than the fastest of the tested
sorting algorithms for all distributions.
It however only supports the \texttt{int32}
combination.


\section{Runtime with respect to the number of elements}

In this section, the runtime per element with respect to the number of elements
for different distributions, key payload data type combinations and
sorting algorithms is analyzed.
Only some graphs are shown in this section, since most of the graphs
are very similar, apart from the ordering of the algorithms by runtime,
which was already discussed in the previous section.
%Some additional graphs can be found in appendix~\ref{}.

\tpeFig{int32}{Uniform}

As an example, figure~\ref{fig:tpe-int32-Uniform} shows the runtime
per element with respect to
the number of elements for the Uniform distribution and the key payload data type
combination \texttt{int32} for different sorting algorithms.
When sorting small arrays (less than $2^4=16$ elements), the runtime per
element is largest and approximately the same for all sorting algorithms
except for IPPRadix where the runtime is significantly higher than the runtime
of the other algorithms for small arrays.

As the number of elements gets larger, the runtime per element decreases for every
algorithm up until about $2^{6}=64$ elements where the runtime per element
plateaus for most of the algorithms. Between approximately $2^{10}=1024$ and
$2^{12}=4096$ elements, the runtime per element gets larger as the number
of elements increases for almost all of the algorithms. From that point on,
the runtime per element gets larger still, but the slope of the curve is
less steep than between $2^{10}=1024$ and $2^{12}=4096$ elements.

The initially high and then decreasing runtime per element is probably due
to the fact that the calling overhead is constant and
thus very high in relation to the number of elements for small arrays.

The plateau and then rise in runtime per element is probably caused by
caching effects.


\tpeFig{int32}{Zero}

Figure~\ref{fig:tpe-int32-Zero} shows the runtime per element of
different sorting algorithm for the Zero
distribution and the key payload data type combination \texttt{int32}
as another example.

Here one can clearly see the runtime complexity of $\mathcal{O}(n^2)$
of BramasSort for constant input, which becomes visible for more than $2^8$ elements,
since below that threshold BramasSort uses bitonic sort with better runtime complexity
for constant input.

Additionally, BlacherSort has a very low runtime for more
than $2^9$ elements.
This is very likely due to the early termination enabled by
keeping track of the smallest and largest value while partitioning,
as mentioned above.


\section{Runtime of RadixSIMDNoCmp with respect to the threshold}

In this section, the runtime of RadixSIMDNoCmp with respect to the threshold
\texttt{cmpSortThreshold}
for different distributions and key payload data type combinations is analyzed.

As in section~\ref{sec:determining-the-optimal-comparison-sorter-threshold},
the size of the array did not seem to have an impact on the behavior (apart
from the absolute runtimes). Thus, only the data where the array size is
$2^{18}=262144$ is shown.
Additionally, the graphs for the distributions Sorted, ReverseSorted,
AlmostSorted and AlmostReverseSorted are not shown either, since they are
qualitatively the same as the graph for the Uniform distribution.

\newcommand{\nocmpfig}[2]{
  \begin{figure}[ht!]
    \centering
    \begin{tikzpicture}
      \begin{axis}[
          ylabel={time per element [ns]},
          xlabel={threshold},
          height=0.35\textheight,
          width=\textwidth,
          xminorgrids=true,
          xmajorgrids=true,
          yminorgrids=true,
          ymajorgrids=true,
          xmode=log,
          log basis x=2,
          %xmin=2,
          %ymax=#2,
          %xtick={2^1,2^2,2^3,2^4,2^5,2^6,2^7,2^8,2^9},
          legend style={fill=white, fill opacity=0.6, draw opacity=1,text opacity=1,font=\footnotesize},
          legend cell align={left},
          legend pos= north east,
        ]
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-float-RadixSIMDNoCmp-#1.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-float-int32-RadixSIMDNoCmp-#1.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-double-RadixSIMDNoCmp-#1.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-double-int64-RadixSIMDNoCmp-#1.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int8-RadixSIMDNoCmp-#1.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int16-RadixSIMDNoCmp-#1.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int32-RadixSIMDNoCmp-#1.dat};
        \addplot table[x=cmpThresh, y=262144] {data/cmpThresh-int64-RadixSIMDNoCmp-#1.dat};
        \legend{\texttt{float}, \texttt{float-int32}, \texttt{double}, \texttt{double-int64}, \texttt{int8}, \texttt{int16}, \texttt{int32}, \texttt{int64}}
      \end{axis}
    \end{tikzpicture}
    \caption{Runtime of RadixSIMDNoCmp for different values for \texttt{cmpSortThreshold}, $2^{18}$ elements, #1 distribution}
    \label{fig:cmpThresh-RadixSIMDNoCmp-#1}
  \end{figure}
}

\nocmpfig{Uniform}{30}

Figure~\ref{fig:cmpThresh-RadixSIMDNoCmp-Uniform} shows the runtime of RadixSIMDNoCmp
for the Uniform distribution. As one might expect, the runtime per element
decreases with increasing threshold, since the sorting tree gets less deep.
When the threshold is set to the size of the array ($2^{18}$), the runtime per
element is almost 0, as array is already at the threshold at the beginning of the
sorting and the sorting algorithm immediately terminates.

For small thresholds below about $2^3$, the decrease in runtime per element as the threshold
increases is larger than for thresholds about $2^3$, where the decrease looks
linear (with a logarithmic x-axis).
This is probably because the cost for each recursive call is constant,
and for smaller subarrays there are less elements sorted per recursive
call, making the cost for each recursive call larger per element.


\nocmpfig{Gaussian}{30}

Figure~\ref{fig:cmpThresh-RadixSIMDNoCmp-Gaussian} shows the runtime of RadixSIMDNoCmp
for the Gaussian distribution. The results are similar to the Uniform distribution.

\nocmpfig{Zero}{70}
\nocmpfig{ZeroOne}{70}

Figures~\ref{fig:cmpThresh-RadixSIMDNoCmp-Zero} and
\ref{fig:cmpThresh-RadixSIMDNoCmp-ZeroOne} show the runtime of RadixSIMDNoCmp
for the Zero and ZeroOne distributions. Apart from the case where
the threshold is equal to the size of the array,
the threshold does not seem to influence the runtime per element
for these distributions.
This is because the elements are all equal (Zero distribution)
or only differ in the lowest bit (ZeroOne distribution).
This causes one of the two subarrays produced by the bit sorter to be empty,
while the other one is the same size as the input array.
Thus, the subarrays never get smaller and therefore never reach
the threshold, independent of the actual value of the threshold.


\chapter{Discussion}

\section{Discussion of the results}

The results presented in chapter~\ref{chap:results} show that the goal of this
thesis was achieved to a large extent.
For most of the key payload data type combinations and most distributions,
the presented implementation of MSB Radix Sort with separate key and payload
datastreams (RadixSIMD) is faster or equally as fast as the
implementation of MSB Radix Sort with combined key and payload
datastreams by \citet{moeller_radix} (MoellerCompress).
As the comparison between the two algorithms with and without payloads shows,
this is at least partly due to the separation of the key and payload
datastreams when sorting datasets with payload.
Another reason for the speedup is likely due to the difference
in the handling of remaining elements in the bit sorter.

For datasets with payload with Zero or ZeroOne distribution, RadixSIMD
is slower than MoellerCompress. The reason for this is not clear and might
be worth investigating.

The algorithm MoellerCompress by \citet{moeller_radix} was already
significantly faster than STLSort (the sorting algorithm included in the C++ standard
template library \citep{enwiki:stl_sort}), which was verified in this thesis.
The algorithm RadixSIMD developed in this thesis provides an additional
speedup over STLSort.

The comparisons with the sorting algorithms RadixSIMDBramSmall and RadixSIMDNoCmp
show that the algorithm RadixSIMD spends a large portion of the sorting
time in the comparison sorter for small subarrays and thus can be
accelerated significantly with just improving the comparison sorter or
using a better one.

The algorithms IPPRadix and BramasSort \citep{bramas} outperformed the algorithms
developed by \citet{moeller_radix} and in this thesis significantly
for many tested scenarios while BlacherSort \citep{blacher} outperformed all tested
algorithms for almost all tested scenarios.
However, IPPRadix does not support payloads, BramasSort only supports the combinations
\texttt{int32}, \texttt{double} and \texttt{int32-int32} and BlacherSort
only supports the combination \texttt{int32}, while
RadixSIMD supports arbitrary keys and payloads.


\section{Possible improvements}

\subsection{Caching more vectors}

For datasets where the key is smaller than one of the payloads, the use of
larger emulated vector registers for the payloads and thus the caching of
more vectors as described in section~\ref{sec:choosing_numElemsPerVec}
(the emulated vector registers consist of multiple
physical vector registers, see section~\ref{sec:template_wrapper})
provides a significant speedup, as seen in
section~\ref{sec:comparingRadixSIMDAndRadixSIMDOneReg}.

It may be beneficial to cache even more vectors by using the emulated
vectors for the keys as well, not just for datasets where the key is
smaller than one of the payloads. This would require larger emulated
masks.
However, the speedup for the method described in section~\ref{sec:choosing_numElemsPerVec}
is probably only caused by the fact that the keys are still stored in only
a single physical register and thus the bit tests can be executed with one
instruction, but the vector register is larger, allowing more keys
to be sorted simultaneously. Caching even more vectors and using the
emulated vectors for the keys as well would require the bit testing of the
keys to be split into multiple instructions
(one for each physical part of the emulated vector register)
and thus might not provide additional speedup.

Nevertheless, caching more vectors might accelerate the sorting
algorithm because of other effects (such as more efficient cache usage or
enabling better instruction-level parallelism).

It would be interesting to see what happens when the algorithm uses
more vector registers than are available on the processor
and thus has to swap vector into memory.

\subsection{Faster sorting algorithm for small subarrays}

As seen in section~\ref{sec:comparison-other-sorting-algorithms},
RadixSIMD can be accelerated greatly by using a faster comparison
sorter for small subarrays. There may be an algorithm that performs
even better than the small sort by \citet{bramas} used
by RadixSIMDBramSmall.
A faster comparison sorter might also allow the threshold \texttt{cmpSortThreshold}
to be increased and thus making the sorting tree of the MSB Radix Sort
shallower, which may further increase performance.

\subsection{Keeping track of smallest and largest element for early termination}

The AVX2 Quicksort implementation by \citet{blacher} keeps track of the
smallest and largest element in the subarray when partitioning.
By comparing these two elements with the pivot, it is possible to
detect if all the elements in either of the two resulting
subarrays are the same. If this is the case, the sorting
of the respective subarray can be skipped. This improves
the sorting time for arrays with many duplicate elements significantly,
as can be seen in section~\ref{sec:comparison-other-sorting-algorithms}
or the graphs in appendix~\ref{appendix:graphs}.

It should be possible to implement this for the MSB Radix Sort as well, which
would probably accelerate the sorting of arrays with many duplicate
elements.

When sorting arrays with few or without any duplicates however, this modification
should not lead to early termination, but the algorithm still has to
keep track of the smallest and largest element.
So, if this were to be implemented, the performance
when sorting arrays with no duplicates should be analyzed as well.

\subsection{AVX/AVX2 version}

The implementation of MSB Radix Sort presented in this thesis uses
the AVX-512 instruction set and for some key payload data type
combinations it requires some AVX-512 extensions
that not all CPUs with AVX-512 support have.

For CPUs that do not support AVX-512, it might be possible to implement an
AVX/AVX2 version of the algorithm by simulating \texttt{mask\_compressstoreu}
instructions in a similar way \citet{blacher} did with his AVX2 implementation
of Quicksort.

\subsection{Multithreading}

The current implementation of the algorithm only runs on a single thread.
But the two subarrays the bit sorter produces can be sorted independent
of each other which allows them to be sorted simultaneously
by multiple threads. On CPUs with multiple cores this
very likely improves the performance of the algorithm significantly.

However, to produce the two subarrays the bit sorter has run first,
so the two subarrays are not available immediately to be sorted
by multiple threads and only one thread is used in the beginning.
There might be a way to parallelize the bit sorting to
benefit from multiple threads early in the sorting process.

\subsection{Quicksort instead of MSB Radix Sort}

MSB Radix Sort can be interpreted as a variant of Quicksort, where
the pivot is a `virtual' element with exactly one bit set
\citep[p. 128]{knuth_taocp}. By actually using Quicksort
instead of MSB Radix Sort, a different pivot could be chosen
such that the sorting tree is more balanced.
For example, the implementation by \citet{blacher} seems to
choose a pivot very well, as this sorting algorithm
outperforms all other tested algorithm with almost all
distributions (see section~\ref{sec:comparison-other-sorting-algorithms}).

\subsection{Additional experiments}

In this thesis it was not analyzed how the runtime of the algorithm
depends on the number and size of the payloads.

One might expect, that the runtime increases linearly with the number
as well as the size of the payloads at least until there are not enough
vector registers available to hold a key vector and a vector for each
payload. It would be interesting to see what happens if there
are not enough vector registers available and the vectors have to be
swapped into memory.

Additionally, multiple smaller payloads could be arranged into multiple payload
arrays (SoA) but could also be combined into one bigger payload,
essentially interleaving the multiple smaller payloads into one array (AoS).
It would be interesting to analyze which of these two options is faster.\\

The behavior of the algorithm for distributions with no duplicate
keys was not analyzed in this thesis either, as most of the distributions
used in the experiments in this thesis likely
had duplicate keys. It might be worthwhile to test if there
is a difference in the runtime of the algorithms when the
distributions have no duplicates.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\chapter{C++ code for the SIMD bit sorter class}
\label{appendix:bit_sorter_code}

\begin{minted}[
    frame=lines,
    framesep=2mm,
    fontsize=\fontsize{9pt}{9pt},
    linenos,
    bgcolor=white,
    numbersep=5pt
    ]{c++}
template <bool OneReg = false> struct BitSorterSIMD {
  template <typename K, typename... Ps>
  static constexpr SortIndex
      numElemsPerVec = OneReg ? 64 / std::max({sizeof(K), sizeof(Ps)...})
                              : 64 / sizeof(K);

  template <bool Up, bool IsHighestBit, bool IsRightSide, typename K,
            typename... Ps>
  static INLINE SortIndex sort(int bitNo, SortIndex left, SortIndex right,
                               K *keys, Ps *...payloads) {
    static constexpr SortIndex _numElemsPerVec = numElemsPerVec<K, Ps...>;

    SortIndex numElems = right - left + 1;

    SortIndex readPosLeft = left;
    SortIndex readPosRight = right - _numElemsPerVec + 1;
    SortIndex writePosLeft = left;
    SortIndex writePosRight = right;

    simd::Vec<K, _numElemsPerVec * sizeof(K)> keyVecStore;
    std::tuple<simd::Vec<Ps, _numElemsPerVec * sizeof(Ps)>...> payloadVecStore;
    if (numElems >= _numElemsPerVec) {
      keyVecStore =
          simd::loadu<_numElemsPerVec * sizeof(K)>(&keys[readPosLeft]);
      payloadVecStore = std::make_tuple(
          simd::loadu<_numElemsPerVec * sizeof(Ps)>(&payloads[readPosLeft])...);
      readPosLeft += _numElemsPerVec;
    }

    while (readPosLeft <= readPosRight) {
      auto keyVec = keyVecStore;
      auto payloadVec = payloadVecStore;
      auto [sortMaskLeft, sortMaskRight] =
          getSortMasks<Up, IsHighestBit, IsRightSide, K, Ps...>(keyVec, bitNo);
      SortIndex numElemsToLeft = simd::kpopcnt(sortMaskLeft);
      SortIndex numElemsToRight = _numElemsPerVec - numElemsToLeft;
      bool areEnoughElemsFreeLeft =
          (readPosLeft - writePosLeft) >= numElemsToLeft;
      if (areEnoughElemsFreeLeft) {
        keyVecStore =
            simd::loadu<_numElemsPerVec * sizeof(K)>(&keys[readPosRight]);
        payloadVecStore =
            std::make_tuple(simd::loadu<_numElemsPerVec * sizeof(Ps)>(
                &payloads[readPosRight])...);
        readPosRight -= _numElemsPerVec;
      } else {
        keyVecStore =
            simd::loadu<_numElemsPerVec * sizeof(K)>(&keys[readPosLeft]);
        payloadVecStore =
            std::make_tuple(simd::loadu<_numElemsPerVec * sizeof(Ps)>(
                &payloads[readPosLeft])...);
        readPosLeft += _numElemsPerVec;
      }
      compress_store_left_right(
          writePosLeft, writePosRight - numElemsToRight + 1, sortMaskLeft,
          sortMaskRight, keyVec, payloadVec, keys, payloads...);
      writePosLeft += numElemsToLeft;
      writePosRight -= numElemsToRight;
    }

    SortIndex numElemsRest = readPosRight + _numElemsPerVec - readPosLeft;

    simd::Mask<_numElemsPerVec> restMask;
    simd::Vec<K, _numElemsPerVec * sizeof(K)> keyVecRest;
    std::tuple<simd::Vec<Ps, _numElemsPerVec * sizeof(Ps)>...> payloadVecRest;
    if (numElemsRest != 0) {
      restMask = (simd::knot(simd::Mask<_numElemsPerVec>(0))) >>
                 (_numElemsPerVec - numElemsRest);
      keyVecRest = simd::maskz_loadu<_numElemsPerVec * sizeof(K)>(
          restMask, &keys[readPosLeft]);
      payloadVecRest =
          std::make_tuple(simd::maskz_loadu<_numElemsPerVec * sizeof(Ps)>(
              restMask, &payloads[readPosLeft])...);
      readPosLeft += numElemsRest;
    }

    if (numElems >= _numElemsPerVec) {
      auto [sortMaskLeft, sortMaskRight] =
          getSortMasks<Up, IsHighestBit, IsRightSide, K, Ps...>(keyVecStore,
                                                                bitNo);
      SortIndex numElemsToLeft = simd::kpopcnt(sortMaskLeft);
      SortIndex numElemsToRight = _numElemsPerVec - numElemsToLeft;
      compress_store_left_right(
          writePosLeft, writePosRight - numElemsToRight + 1, sortMaskLeft,
          sortMaskRight, keyVecStore, payloadVecStore, keys, payloads...);
      writePosLeft += numElemsToLeft;
      writePosRight -= numElemsToRight;
    }

    if (numElemsRest != 0) {
      auto [sortMaskLeftRest, sortMaskRightRest] =
          getSortMasks<Up, IsHighestBit, IsRightSide, K, Ps...>(keyVecRest,
                                                                bitNo);
      sortMaskLeftRest = simd::kand(sortMaskLeftRest, restMask);
      sortMaskRightRest = simd::kand(sortMaskRightRest, restMask);
      SortIndex numElemsToLeftRest = simd::kpopcnt(sortMaskLeftRest);
      SortIndex numElemsToRightRest = numElemsRest - numElemsToLeftRest;
      compress_store_left_right(writePosLeft, writePosLeft + numElemsToLeftRest,
                                sortMaskLeftRest, sortMaskRightRest, keyVecRest,
                                payloadVecRest, keys, payloads...);
      writePosLeft += numElemsToLeftRest;
      writePosRight -= numElemsToRightRest;
    }
    return writePosLeft;
  }

private:
  template <bool Up, bool IsHighestBit, bool IsRightSide, typename K,
            typename... Ps>
  static INLINE std::tuple<simd::Mask<numElemsPerVec<K, Ps...>>,
                           simd::Mask<numElemsPerVec<K, Ps...>>>
  getSortMasks(simd::Vec<K, numElemsPerVec<K, Ps...> * sizeof(K)> keyVec,
               int bitNo) {
    auto bitMaskVec = simd::reinterpret<K>(
        simd::set1<UInt<sizeof(K)>, numElemsPerVec<K, Ps...> * sizeof(K)>(
            UInt<sizeof(K)>(1) << bitNo));
    if constexpr (bitDirUp<K, Up, IsHighestBit, IsRightSide>()) {
      auto sortMaskRight = simd::test_mask(keyVec, bitMaskVec);
      auto sortMaskLeft = simd::knot(sortMaskRight);
      return std::make_tuple(sortMaskLeft, sortMaskRight);
    } else {
      auto sortMaskLeft = simd::test_mask(keyVec, bitMaskVec);
      auto sortMaskRight = simd::knot(sortMaskLeft);
      return std::make_tuple(sortMaskLeft, sortMaskRight);
    }
  }

  template <typename K, typename... Ps>
  static INLINE void compress_store_left_right(
      SortIndex leftPos, SortIndex rightPos,
      simd::Mask<numElemsPerVec<K, Ps...>> leftMask,
      simd::Mask<numElemsPerVec<K, Ps...>> rightMask,
      simd::Vec<K, numElemsPerVec<K, Ps...> * sizeof(K)> keyVec,
      std::tuple<simd::Vec<Ps, numElemsPerVec<K, Ps...> * sizeof(Ps)>...>
          payloadVec,
      K *keys, Ps *...payloads) {

    simd::mask_compressstoreu(&keys[leftPos], leftMask, keyVec);
    std::apply(
        [&](auto... payloadVecs) {
          (simd::mask_compressstoreu(&payloads[leftPos], leftMask, payloadVecs),
           ...);
        },
        payloadVec);

    simd::mask_compressstoreu(&keys[rightPos], rightMask, keyVec);
    std::apply(
        [&](auto... payloadVecs) {
          (simd::mask_compressstoreu(&payloads[rightPos], rightMask,
                                     payloadVecs),
           ...);
        },
        payloadVec);
  }
};
\end{minted}

\chapter{Comparison graphs}
\label{appendix:graphs}
The bars in the following figures
(\ref{fig:tpeBar-Uniform-int32-appendix}-\ref{fig:tpeBar-AlmostReverseSorted-int16-appendix})
are colored to make the graphs easier to read.
The bars for the sequential algorithms are colored black,
RadixSIMD is colored red and
RadixSIMDBramSmall is colored brown.
The bars for the rest of the algorithms are colored blue.
The bar for RadixSIMDNoCmp is dashed as a reminder that it does not
actually fully sort the data.
\clearpage
%\cleardoublepage
%\pagenumbering{Roman}
%\setcounter{page}{\value{romanpage}}

\section{\texttt{int32}}

\tpeBarxmax{Uniform}{int32}{-appendix}{70}
\tpeBarxmax{Gaussian}{int32}{-appendix}{40}
\tpeBarxmax{Zero}{int32}{-appendix}{8}
\tpeBarxmax{ZeroOne}{int32}{-appendix}{12}
\tpeBarxmax{Sorted}{int32}{-appendix}{10}
\tpeBarxmax{ReverseSorted}{int32}{-appendix}{12}
\tpeBarxmax{AlmostSorted}{int32}{-appendix}{10}
\tpeBarxmax{AlmostReverseSorted}{int32}{-appendix}{22}

\section{\texttt{int32-int32}}

\tpeBarxmax{Uniform}{int32-int32}{-appendix}{70}
\tpeBarxmax{Gaussian}{int32-int32}{-appendix}{55}
\tpeBarxmax{Zero}{int32-int32}{-appendix}{15}
\tpeBarxmax{ZeroOne}{int32-int32}{-appendix}{15}
\tpeBarxmax{Sorted}{int32-int32}{-appendix}{12}
\tpeBarxmax{ReverseSorted}{int32-int32}{-appendix}{18}
\tpeBarxmax{AlmostSorted}{int32-int32}{-appendix}{11}
\tpeBarxmax{AlmostReverseSorted}{int32-int32}{-appendix}{18}

\section{\texttt{float}}

\tpeBarxmax{Uniform}{float}{-appendix}{70}
\tpeBarxmax{Gaussian}{float}{-appendix}{70}
\tpeBarxmax{Zero}{float}{-appendix}{8}
\tpeBarxmax{ZeroOne}{float}{-appendix}{12}
\tpeBarxmax{Sorted}{float}{-appendix}{11}
\tpeBarxmax{ReverseSorted}{float}{-appendix}{13}
\tpeBarxmax{AlmostSorted}{float}{-appendix}{12}
\tpeBarxmax{AlmostReverseSorted}{float}{-appendix}{37}

\section{\texttt{float-int32}}

\tpeBarxmax{Uniform}{float-int32}{-appendix}{70}
\tpeBarxmax{Gaussian}{float-int32}{-appendix}{70}
\tpeBarxmax{Zero}{float-int32}{-appendix}{14}
\tpeBarxmax{ZeroOne}{float-int32}{-appendix}{12}
\tpeBarxmax{Sorted}{float-int32}{-appendix}{14}
\tpeBarxmax{ReverseSorted}{float-int32}{-appendix}{15}
\tpeBarxmax{AlmostSorted}{float-int32}{-appendix}{14}
\tpeBarxmax{AlmostReverseSorted}{float-int32}{-appendix}{17}



\section{\texttt{double}}

\tpeBarxmax{Uniform}{double}{-appendix}{70}
\tpeBarxmax{Gaussian}{double}{-appendix}{70}
\tpeBarxmax{Zero}{double}{-appendix}{18}
\tpeBarxmax{ZeroOne}{double}{-appendix}{20}
\tpeBarxmax{Sorted}{double}{-appendix}{16}
\tpeBarxmax{ReverseSorted}{double}{-appendix}{18}
\tpeBarxmax{AlmostSorted}{double}{-appendix}{17}
\tpeBarxmax{AlmostReverseSorted}{double}{-appendix}{29}



\section{\texttt{uint8}}

\tpeBarxmax{Uniform}{uint8}{-appendix}{1}
\tpeBarxmax{Gaussian}{uint8}{-appendix}{1}
\tpeBarxmax{Zero}{uint8}{-appendix}{4.5}
\tpeBarxmax{ZeroOne}{uint8}{-appendix}{9}
\tpeBarxmax{Sorted}{uint8}{-appendix}{4.5}
\tpeBarxmax{ReverseSorted}{uint8}{-appendix}{4.5}
\tpeBarxmax{AlmostSorted}{uint8}{-appendix}{4.5}
\tpeBarxmax{AlmostReverseSorted}{uint8}{-appendix}{4.5}

\section{\texttt{int16}}

\tpeBarxmax{Uniform}{int16}{-appendix}{67}
\tpeBarxmax{Gaussian}{int16}{-appendix}{37}
\tpeBarxmax{Zero}{int16}{-appendix}{5.5}
\tpeBarxmax{ZeroOne}{int16}{-appendix}{11}
\tpeBarxmax{Sorted}{int16}{-appendix}{9}
\tpeBarxmax{ReverseSorted}{int16}{-appendix}{10}
\tpeBarxmax{AlmostSorted}{int16}{-appendix}{9}
\tpeBarxmax{AlmostReverseSorted}{int16}{-appendix}{22}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
\stepcounter{chapter}
\bibliographystyle{ti_plainnat2}
\bibliography{\jobname}
\addcontentsline{toc}{chapter}{\bibname}
\nocite{*}
\end{document}
